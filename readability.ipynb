{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# todo\n",
    "_fixRelativeUris\n",
    "_simplifyNestedElements\n",
    "_cleanClasses\n",
    "_prepArticle\n",
    "hasSingleTagInsideElement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, element\n",
    "from html import unescape\n",
    "import re\n",
    "import requests\n",
    "\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 1297)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:1297\u001b[0;36m\u001b[0m\n\u001b[0;31m    },\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "class Readability:\n",
    "    def __init__(self, doc, options={}):\n",
    "        self.doc = doc\n",
    "        # self.FLAG_STRIP_UNLIKELYS= \"0x1\" # hex\n",
    "        # self.FLAG_WEIGHT_CLASSES= \"0x2\" # hex\n",
    "        # self.FLAG_CLEAN_CONDITIONALLY= \"0x4\" # hex\n",
    "        self.FLAG_STRIP_UNLIKELYS = 0x1\n",
    "        self.FLAG_WEIGHT_CLASSES = 0x2\n",
    "        self.FLAG_CLEAN_CONDITIONALLY = 0x4\n",
    "\n",
    "        #   // https://developer.mozilla.org/en-US/docs/Web/API/Node/nodeType\n",
    "        self.ELEMENT_NODE= 1,\n",
    "        self.TEXT_NODE= 3,\n",
    "\n",
    "        #   // Max number of nodes supported by this parser. Default: 0 (no limit)\n",
    "        self.DEFAULT_MAX_ELEMS_TO_PARSE= 0\n",
    "\n",
    "        #   // The number of top candidates to consider when analysing how\n",
    "        #   // tight the competition is among candidates.\n",
    "        self.DEFAULT_N_TOP_CANDIDATES= 5\n",
    "\n",
    "        # // Element tags to score by default.\n",
    "        self.DEFAULT_TAGS_TO_SCORE= \"section,h2,h3,h4,h5,h6,p,td,pre\".upper().split(\",\"),\n",
    "\n",
    "        # // The default number of chars an article must have in order to return a result\n",
    "        self.DEFAULT_CHAR_THRESHOLD= 500\n",
    "\n",
    "        # // All of the regular expressions in use within readability.\n",
    "        # // Defined up here so we don't instantiate them repeatedly in loops.\n",
    "        self.REGEXPS = {\n",
    "            # NOTE: These two regular expressions are duplicated in\n",
    "            # Readability-readerable.js. Please keep both copies in sync.\n",
    "            'unlikelyCandidates': re.compile(r'-ad-|ai2html|banner|breadcrumbs|combx|comment|community|cover-wrap|disqus|extra|footer|gdpr|header|legends|menu|related|remark|replies|rss|shoutbox|sidebar|skyscraper|social|sponsor|supplemental|ad-break|agegate|pagination|pager|popup|yom-remote', re.IGNORECASE),\n",
    "            'okMaybeItsACandidate': re.compile(r'and|article|body|column|content|main|shadow', re.IGNORECASE),\n",
    "\n",
    "            'positive': re.compile(r'article|body|content|entry|hentry|h-entry|main|page|pagination|post|text|blog|story', re.IGNORECASE),\n",
    "            'negative': re.compile(r'-ad-|hidden|^hid$| hid$| hid |^hid |banner|combx|comment|com-|contact|foot|footer|footnote|gdpr|masthead|media|meta|outbrain|promo|related|scroll|share|shoutbox|sidebar|skyscraper|sponsor|shopping|tags|tool|widget', re.IGNORECASE),\n",
    "            'extraneous': re.compile(r'print|archive|comment|discuss|e[\\-]?mail|share|reply|all|login|sign|single|utility', re.IGNORECASE),\n",
    "            'byline': re.compile(r'byline|author|dateline|writtenby|p-author', re.IGNORECASE),\n",
    "            'replaceFonts': re.compile(r'<(\\/?)font[^>]*>', re.IGNORECASE),\n",
    "            'normalize': re.compile(r'\\s{2,}'),\n",
    "            'videos': re.compile(r'\\/\\/(www\\.)?((dailymotion|youtube|youtube-nocookie|player\\.vimeo|v\\.qq)\\.com|(archive|upload\\.wikimedia)\\.org|player\\.twitch\\.tv)', re.IGNORECASE),\n",
    "            'shareElements': re.compile(r'(\\b|_)(share|sharedaddy)(\\b|_)', re.IGNORECASE),\n",
    "            'nextLink': re.compile(r'(next|weiter|continue|>([^\\|]|$)|»([^\\|]|$))', re.IGNORECASE),\n",
    "            'prevLink': re.compile(r'(prev|earl|old|new|<|«)', re.IGNORECASE),\n",
    "            'tokenize': re.compile(r'\\W+'),\n",
    "            'whitespace': re.compile(r'^\\s*$'),\n",
    "            'hasContent': re.compile(r'\\S$'),\n",
    "            'hashUrl': re.compile(r'^#.+'),\n",
    "            'srcsetUrl': re.compile(r'(\\S+)(\\s+[\\d.]+[xw])?(\\s*(?:,|$))'),\n",
    "            'b64DataUrl': re.compile(r'^data:\\s*([^\\s;,]+)\\s*;\\s*base64\\s*,', re.IGNORECASE),\n",
    "            # See: https://schema.org/Article\n",
    "            'jsonLdArticleTypes': re.compile(r'^Article|AdvertiserContentArticle|NewsArticle|AnalysisNewsArticle|AskPublicNewsArticle|BackgroundNewsArticle|OpinionNewsArticle|ReportageNewsArticle|ReviewNewsArticle|Report|SatiricalArticle|ScholarlyArticle|MedicalScholarlyArticle|SocialMediaPosting|BlogPosting|LiveBlogPosting|DiscussionForumPosting|TechArticle|APIReference$'),\n",
    "        }\n",
    "\n",
    "        self.UNLIKELY_ROLES = [ \"menu\", \"menubar\", \"complementary\", \"navigation\", \"alert\", \"alertdialog\", \"dialog\" ]\n",
    "\n",
    "        self.DIV_TO_P_ELEMS =  {\"BLOCKQUOTE\", \"DL\", \"DIV\", \"IMG\", \"OL\", \"P\", \"PRE\", \"TABLE\", \"UL\"}\n",
    "\n",
    "        self.ALTER_TO_DIV_EXCEPTIONS = [\"DIV\", \"ARTICLE\", \"SECTION\", \"P\"]\n",
    "\n",
    "        self.PRESENTATIONAL_ATTRIBUTES = [ \"align\", \"background\", \"bgcolor\", \"border\", \"cellpadding\", \"cellspacing\", \"frame\", \"hspace\", \"rules\", \"style\", \"valign\", \"vspace\" ]\n",
    "\n",
    "        self.DEPRECATED_SIZE_ATTRIBUTE_ELEMS = [ \"TABLE\", \"TH\", \"TD\", \"HR\", \"PRE\" ]\n",
    "\n",
    "        # // The commented out elements qualify as phrasing content but tend to be\n",
    "        # // removed by readability when put into paragraphs, so we ignore them here.\n",
    "        self.PHRASING_ELEMS = [\n",
    "            # // \"CANVAS\", \"IFRAME\", \"SVG\", \"VIDEO\",\n",
    "            \"ABBR\", \"AUDIO\", \"B\", \"BDO\", \"BR\", \"BUTTON\", \"CITE\", \"CODE\", \"DATA\",\n",
    "            \"DATALIST\", \"DFN\", \"EM\", \"EMBED\", \"I\", \"IMG\", \"INPUT\", \"KBD\", \"LABEL\",\n",
    "            \"MARK\", \"MATH\", \"METER\", \"NOSCRIPT\", \"OBJECT\", \"OUTPUT\", \"PROGRESS\", \"Q\",\n",
    "            \"RUBY\", \"SAMP\", \"SCRIPT\", \"SELECT\", \"SMALL\", \"SPAN\", \"STRONG\", \"SUB\",\n",
    "            \"SUP\", \"TEXTAREA\", \"TIME\", \"VAR\", \"WBR\"\n",
    "        ]\n",
    "        \n",
    "\n",
    "        # // These are the classes that readability sets itself.\n",
    "        self.CLASSES_TO_PRESERVE = [ \"page\" ]\n",
    "\n",
    "        # // These are the list of HTML entities that need to be escaped.\n",
    "        self.HTML_ESCAPE_MAP = {\n",
    "            \"lt\": \"<\",\n",
    "            \"gt\": \">\",\n",
    "            \"amp\": \"&\",\n",
    "            \"quot\": '\"',\n",
    "            \"apos\": \"'\",\n",
    "        }\n",
    "\n",
    "        self._doc = BeautifulSoup(doc, \"html.parser\") # doc\n",
    "        # self._docJSDOMParser = self._doc.firstChild.__JSDOMParser__; # Needs work\n",
    "        self._articleTitle = None\n",
    "        self._articleByline = None\n",
    "        self._articleDir = None\n",
    "        self._articleSiteName = None\n",
    "        self._attempts = []\n",
    "\n",
    "        # // Configurable options\n",
    "        self._debug = options.get(\"debug\", True) # Does not originally have a default\n",
    "        \n",
    "    \n",
    "        self._maxElemsToParse = options.get(\"maxElemsToParse\", self.DEFAULT_MAX_ELEMS_TO_PARSE)\n",
    "        self._nbTopCandidates = options.get(\"nbTopCandidates\", self.DEFAULT_N_TOP_CANDIDATES)\n",
    "        self._charThreshold = options.get(\"charThreshold\", self.DEFAULT_CHAR_THRESHOLD)\n",
    "        self._classesToPreserve = self.CLASSES_TO_PRESERVE + options.get(\"classesToPreserve\", [])\n",
    "        self._keepClasses = options.get(\"keepClasses\", True) # Does not originally have a default\n",
    "        self.serializer = options.get(\"serializer\", lambda el: el[\"innterHTML\"])\n",
    "        self._disableJSONLD = options.get(\"disableJSONLD\", False) # Does not originally have a default\n",
    "\n",
    "        # // Start with all flags set\n",
    "        # self._flags = hex(self.FLAG_STRIP_UNLIKELYS |\n",
    "        #                 self.FLAG_WEIGHT_CLASSES |\n",
    "        #                 self.FLAG_CLEAN_CONDITIONALLY)\n",
    "        self._flags = self.FLAG_STRIP_UNLIKELYS | self.FLAG_WEIGHT_CLASSES | self.FLAG_CLEAN_CONDITIONALLY\n",
    "\n",
    "#     def Readability(self, doc = None, options = {}):\n",
    "#         \"\"\"\n",
    "#         Does not support passing a URI as the first argument\n",
    "# \n",
    "#         Arguments\n",
    "#             * options (dict): \n",
    "#                 * debug (bool)\n",
    "#                 * maxElemsToParse (int?) optional\n",
    "#                 * nbTopCandidates (?) optional\n",
    "#                 * charThreshold (int?) optional\n",
    "#                 * classesToPreserve (?) optional\n",
    "#                 * keepClasses (bool)\n",
    "#                 * serializer (func) optional\n",
    "#                 * disableJSONLD\n",
    "# \n",
    "#             The options object accepts a number of properties, all optional:\n",
    "# \n",
    "#             * debug (boolean, default false): whether to enable logging.\n",
    "#             * maxElemsToParse (number, default 0 i.e. no limit): the maximum number of elements to parse.\n",
    "#             * nbTopCandidates (number, default 5): the number of top candidates to consider when analysing how tight the competition is among candidates.\n",
    "#             * charThreshold (number, default 500): the number of characters an article must have in order to return a result.\n",
    "#             * classesToPreserve (array): a set of classes to preserve on HTML elements when the keepClasses options is set to false.\n",
    "#             * keepClasses (boolean, default false): whether to preserve all classes on HTML elements. When set to false only classes specified in the classesToPreserve array are kept.\n",
    "#             * disableJSONLD (boolean, default false): when extracting page metadata, Readability gives precendence to Schema.org fields specified in the JSON-LD format. Set this option to true to skip JSON-LD parsing.\n",
    "#             * serializer (function, default el => el.innerHTML) controls how the the content property returned by the parse() method is produced from the root DOM element. It may be useful to specify the serializer as the identity function (el => el) to obtain a DOM element instead of a string for content if you plan to process it further.\n",
    "# \n",
    "#         \"\"\"\n",
    "#         if (doc is None):\n",
    "#             raise Exception(\"First argument to Readability constructor should be a document object.\");\n",
    "#     \n",
    "#         # Bad code, fix later\n",
    "#         if \"documentElement\" not in doc:\n",
    "#             raise Exception(\"First argument to Readability constructor should be a document object.\");\n",
    "# \n",
    "#         self._doc = doc;\n",
    "#         self._docJSDOMParser = self._doc.firstChild.__JSDOMParser__; # Needs work\n",
    "#         self._articleTitle = None;\n",
    "#         self._articleByline = None;\n",
    "#         self._articleDir = None;\n",
    "#         self._articleSiteName = None;\n",
    "#         self._attempts = [];\n",
    "# \n",
    "#         # // Configurable options\n",
    "#         self._debug = options[\"debug\"];\n",
    "#         \n",
    "#     \n",
    "#         self._maxElemsToParse = options.get(\"maxElemsToParse\", self.DEFAULT_MAX_ELEMS_TO_PARSE)\n",
    "#         self._nbTopCandidates = options.get(\"nbTopCandidates\", self.DEFAULT_N_TOP_CANDIDATES)\n",
    "#         self._charThreshold = options.get(\"charThreshold\", self.DEFAULT_CHAR_THRESHOLD)\n",
    "#         self._classesToPreserve = this.CLASSES_TO_PRESERVE + options.get(\"classesToPreserve\", [])\n",
    "#         self._keepClasses = options[\"keepClasses\"];\n",
    "#         self.serializer = options.get(\"serializer\", lambda el: el[\"innterHTML\"])\n",
    "#         self._disableJSONLD = options[\"disableJSONLD\"];\n",
    "# \n",
    "#         # // Start with all flags set\n",
    "#         self._flags = hex(self.FLAG_STRIP_UNLIKELYS |\n",
    "#                         self.FLAG_WEIGHT_CLASSES |\n",
    "#                         self.FLAG_CLEAN_CONDITIONALLY)\n",
    "\n",
    "\n",
    "        # // Control whether log messages are sent to the console\n",
    "#         if (self._debug) {\n",
    "#             logNode = lambda node: (\n",
    "#                 if (node[\"nodeType\"] == node[\"TEXT_NODE\"]) {\n",
    "#                     return \"{0} (\\\"{1}\\\")\".format(node[\"nodeName\"], node[\"nodeContext\"])\n",
    "#                 }\n",
    "#                 \n",
    "#                 attributes = node.get(\"attributes\", [])\n",
    "#                 attrPairs = \" \".join([\"{0}=\\\"{1}\\\"\".format(attr[\"name\"], attr[\"value\"]), for attr in node.get(\"attributes\", [])])\n",
    "# \n",
    "#                 return \"<{0} {1}>\".format(node[\"localName\"], attrPairs)\n",
    "#             )\n",
    "# \n",
    "#             this.log = function () {\n",
    "#             if (typeof dump !== \"undefined\") {\n",
    "#                 var msg = Array.prototype.map.call(arguments, function(x) {\n",
    "#                 return (x && x.nodeName) ? logNode(x) : x;\n",
    "#                 }).join(\" \");\n",
    "#                 dump(\"Reader: (Readability) \" + msg + \"\\n\");\n",
    "#             } else if (typeof console !== \"undefined\") {\n",
    "#                 let args = Array.from(arguments, arg => {\n",
    "#                 if (arg && arg.nodeType == this.ELEMENT_NODE) {\n",
    "#                     return logNode(arg);\n",
    "#                 }\n",
    "#                 return arg;\n",
    "#                 });\n",
    "#                 args.unshift(\"Reader: (Readability)\");\n",
    "#                 console.log.apply(console, args);\n",
    "#             }\n",
    "#             };\n",
    "#         } else {\n",
    "#             this.log = function () {};\n",
    "#         }\n",
    "\n",
    "    def _initialize_node(self, node):\n",
    "        node.readability = {\"contentScore\": 0}\n",
    "\n",
    "        tag_name = node.name.lower()\n",
    "\n",
    "        if tag_name == \"div\":\n",
    "            node.readability[\"contentScore\"] += 5\n",
    "        elif tag_name in [\"pre\", \"td\", \"blockquote\"]:\n",
    "            node.readability[\"contentScore\"] += 3\n",
    "        elif tag_name in [\"address\", \"ol\", \"ul\", \"dl\", \"dd\", \"dt\", \"li\", \"form\"]:\n",
    "            node.readability[\"contentScore\"] -= 3\n",
    "        elif tag_name in [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"th\"]:\n",
    "            node.readability[\"contentScore\"] -= 5\n",
    "\n",
    "        node.readability[\"contentScore\"] += self._getClassWeight(node)\n",
    "\n",
    "    def _removeAndGetNext(self, node: element.Tag) -> element.Tag:\n",
    "        nextNode = node.find_next()\n",
    "        node.extract()\n",
    "        return nextNode\n",
    "\n",
    "    def _checkByline(self, node, matchString):\n",
    "        if (self._articleByline):\n",
    "            return False\n",
    "\n",
    "        if hasattr(node, \"attrs\"):\n",
    "            rel = node.attrs.get(\"rel\", \"\")\n",
    "            itemprop = node.attrs.get(\"itemprop\", \"\")\n",
    "\n",
    "        if (rel == \"author\"\n",
    "            or \"authot\" in itemprop\n",
    "            or re.match(self.REGEXPS['byline'], matchString)):\n",
    "            self._articleByline = node.get_text()\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _getNodeAncestors(self, node: element.Tag, maxDepth: int = 0):\n",
    "        i = 0\n",
    "        ancestors = []\n",
    "        while node.parent:\n",
    "            ancestors.appent(node.parent)\n",
    "            i += 1\n",
    "            if maxDepth and i == maxDepth:\n",
    "                break\n",
    "            node = node.parent\n",
    "        return ancestors\n",
    "\n",
    "    def _grabArticle(self, page):\n",
    "        \"\"\"\n",
    "        /***\n",
    "        * grabArticle - Using a variety of metrics (content score, classname, element types), find the content that is\n",
    "        *         most likely to be the stuff a user wants to read. Then return it wrapped up in a div.\n",
    "        *\n",
    "        * @param page a document to run upon. Needs to be a full document, complete with body.\n",
    "        * @return Element\n",
    "        **/\n",
    "        \"\"\"\n",
    "        print(\"**** grabArticle ****\")\n",
    "        isPaging = page != None\n",
    "        page = page if page else self._doc.body\n",
    "\n",
    "        # We can't grab an article if we don't have a page!\n",
    "        if (not page):\n",
    "            print(\"No body found in document. Abort.\")\n",
    "            return\n",
    "        \n",
    "        pageCacheHtml = page\n",
    "        \n",
    "        while True:\n",
    "            print(\"Starting grabArticle loop\")\n",
    "            stripUnlikelyCandidates = self._flagIsActive(self.FLAG_STRIP_UNLIKELYS)\n",
    "\n",
    "            # First, node prepping. Trash nodes that look cruddy (like ones with the\n",
    "            # class name \"comment\", etc), and turn divs into P tags where they have been\n",
    "            # used inappropriately (as in, where they contain no other block level elements.)\n",
    "            elementsToScore = []\n",
    "            node = self._doc.html\n",
    "\n",
    "            shouldRemoveTitleHeader = True\n",
    "\n",
    "            while (node):\n",
    "\n",
    "                if node.name == \"html\":\n",
    "                    self._articleLang = node.attrs.get(\"lang\")\n",
    "                \n",
    "                matchString = \" \".join(node.get(\"class\")) +  \" \" + node.get(\"id\")\n",
    "\n",
    "                if not self._isProbablyVisible(node):\n",
    "                    print(\"Removing hidden node - \" + matchString)\n",
    "                    node = self.removeAndGetNext(node)\n",
    "                    continue\n",
    "\n",
    "                # User is not able to see elements applied with both \"aria-modal = true\" and \"role = dialog\"\n",
    "                if node.attrs.get(\"aria-modal\", \"\") == \"true\" and node.attrs.get(\"role\", \"\") == \"dialog\":\n",
    "                    node = self._removeAndGetNext(node)\n",
    "                    continue\n",
    "\n",
    "                # Check to see if this node is a byline, and remove it if it is\n",
    "                if self._checkByLine(node, matchString):\n",
    "                    node = self._removeAndGetNext(node)\n",
    "                    continue\n",
    "            \n",
    "                if shouldRemoveTitleHeader and self._headerDuplicatesTitle(node):\n",
    "                    print(\"Removing header: \", node.get_text().strip(), self._articleTitle.strip())\n",
    "                    shouldRemoveTitleHeader = false\n",
    "                    node = self._removeAndGetNext(node)\n",
    "                    continue\n",
    "\n",
    "                if stripUnlikelyCandidates:\n",
    "                    if (re.match(self.REGEXPS[\"unlikelyCandidates\"], matchString) \n",
    "                        and not re.match(self.REGEXPS[\"okMaybeItsACandidate\", matchString]) \n",
    "                        and not self._hasAncestorTag(node, \"table\")\n",
    "                        and not self._hasAncestorTag(node, \"code\")\n",
    "                        and node.name != \"body\"\n",
    "                        and node.name != \"a\"\n",
    "                    ):\n",
    "                        print(\"Removing unlikely candidate - \", matchString)\n",
    "                        node = self._removeAndGetNext(node)\n",
    "                        continue\n",
    "\n",
    "                    if node.attrs.get(\"role\") in self.UNLIKELY_ROLES:\n",
    "                        print(\"Removing content with role \", node.attrs.get(\"role\"), \" - \", matchString)\n",
    "                        node = self._removeAndGetNext(node)\n",
    "                        continue\n",
    "\n",
    "                # Remove div, section, and header nodes without any content (e.g. text, iamge, video, or iframe).\n",
    "                if (node.name in [\"div, section, header, h1, h2, h3, h4, h5, h6\"] and self.isElementWithoutContent(node)):\n",
    "                    node = self._removeAndGetNext(node)\n",
    "                    continue\n",
    "                    \n",
    "                if node.name in self.DEFAULT_TAGS_TO_SCORE:\n",
    "                    elementsToScore.append(node)\n",
    "\n",
    "                # Turn all divs that don't have children block level elements into p's\n",
    "                if node.name == \"div\":\n",
    "                    p = None\n",
    "                    childNode = node.find()\n",
    "                    while childNode:\n",
    "                        nextSibling = childNode.next_sibling\n",
    "                        if self._isPhrasingContent(childNode):\n",
    "                            if p is not None:\n",
    "                                p.append(childNode)\n",
    "                            elif not self._isWhitespace(childNode):\n",
    "                                p = self._doc.new_tag(\"p\")\n",
    "                                node.replace_with(p)\n",
    "                                p.append(childNode)\n",
    "                        elif p is not None:\n",
    "                            while p.last_child and self._isWhitespace(p.last_child):\n",
    "                                p.last_child.extract()\n",
    "                            p = None\n",
    "                        childNode = nextSibling\n",
    "\n",
    "                # Sites like http://mobile.slate.com encloses each paragraph with a DIV\n",
    "                # element. DIVs with only a P element inside and no text content can be\n",
    "                # safely converted into plain P elements to avoid confusing the scoring\n",
    "                # algorithm with DIVs with are, in practice, paragraphs.\n",
    "                if (self._hasSingleTagInsideElement(node, \"p\") \n",
    "                    and self._getLinkDensity(node) < 0.25):\n",
    "                    newNode = node.contents[0]\n",
    "                    node.parent.replace_with(newNode)\n",
    "                    node = newNode\n",
    "                    elementsToScore.append(node)\n",
    "\n",
    "                elif not self._hasChildBlockElement(node):\n",
    "                    node = self._setNodeTag(node, \"p\")\n",
    "                    elementsToScore.append(node)\n",
    "                node = self._getNextNode(node)\n",
    "            \n",
    "\n",
    "            # /**\n",
    "            # * Loop through all paragraphs, and assign a score to them based on how content-y they look.\n",
    "            # * Then add their score to their parent node.\n",
    "            # *\n",
    "            # * A score is determined by things like number of commas, class names, etc. Maybe eventually link density.\n",
    "            # **/\n",
    "            candidates = []\n",
    "            for elementToScore in elementsToScore:\n",
    "                if (not elementToScore.parent) or (not elementToScore.parent.name):\n",
    "                    continue\n",
    "\n",
    "                # If this paragraph is less than 25 characters, don't even count it.\n",
    "                innerText = self._getInnerText(elementToScore)\n",
    "                if len(innerText) < 25:\n",
    "                    continue\n",
    "                \n",
    "                # Exclude nodes with no ancestor.\n",
    "                ancestors = self._getNodeAncestors(elementToScore, 5):\n",
    "                if len(ancestors) == 0:\n",
    "                    continue\n",
    "\n",
    "                contentScore = 0\n",
    "\n",
    "                # Add a point for the paragraph itself as a base.\n",
    "                contentScore += 1\n",
    "\n",
    "                # Add points for any commas within this paragraph.\n",
    "                contentScore += len(innerText.split(\",\"))\n",
    "\n",
    "                # For every 100 characters in this paragraph, add another point. Up to 3 points.\n",
    "                contentScore += min(len(innerText) // 100, 3)\n",
    "\n",
    "                # Initialize and score ancestors.\n",
    "                for ancestor, level in zip(ancestors, range(len(ancestors))):\n",
    "                    if (not ancestor.name) or (not ancestor.parent) or (not ancestor.parent.name):\n",
    "                        continue\n",
    "\n",
    "                if not hasattr(ancestor, \"readability\"):\n",
    "                    self.initializeNode(ancestor)\n",
    "                    candidates.append(ancestor)\n",
    "\n",
    "                # Node score divider:\n",
    "                # - parent:             1 (no division)\n",
    "                # - grandparent:        2\n",
    "                # - great grandparent+: ancestor level * 3\n",
    "                if level == 0:\n",
    "                    scoreDivider = 1\n",
    "                elif level == 1:\n",
    "                    scoreDivider = 2\n",
    "                else:\n",
    "                    scoreDivider = level * 3\n",
    "                ancestor.readability[\"contentScore\"] += contentScore / scoreDivider\n",
    "\n",
    "            # After we've calculated scores, loop through all of the possible\n",
    "            # candidate nodes we found and find the one with the highest score.\n",
    "            topCandidates = []\n",
    "            for candidate in candidates:\n",
    "                # Scale the final candidates score based on link density. Good content\n",
    "                # should have a relatively small link density (5% or less) and be mostly\n",
    "                # unaffected by this operation.\n",
    "                candidateScore = candidate.readability[\"contentScore\"] * (1 - self._getLinkDensity(candidate))\n",
    "                candidate.readability[\"contentScore\"] = candidateScore\n",
    "\n",
    "                print(\"Candidate: \", candidate, \" with score \", candidateScore)\n",
    "                for t in range(self._nbTopCandidates):\n",
    "                    aTopCandidate = topCandidates[t]\n",
    "                    if (not aTopCandidate) or (candidateScore > aTopCandidate.readability[\"contentScore\"]): # todo\n",
    "                        topCandidates.insert(t, candidate)\n",
    "                        if len(topCandidates) > self._nbTopCandidates:\n",
    "                            topCandidates.pop()\n",
    "                        break\n",
    "\n",
    "            topCandidate = topCandidates.get(0, None)\n",
    "            neededToCreateTopCandidate = False\n",
    "            parentOfTopCandidate = None\n",
    "\n",
    "            # If we still have no top candidate, just use the body as a last resort.\n",
    "            # We also have to copy the body node so it is something we can modify.\n",
    "            if (topCandidate is None) or (topCandidate.name == \"body\"):\n",
    "                # Move all of the patge's children into topCandidate\n",
    "                topCandidate = self._doc.new_tag(\"div\")\n",
    "                neededToCreateTopCandidate = True\n",
    "                # Move everything (not just elements, also text nodes etc.) into the container\n",
    "                # so we even include text directly in the body\n",
    "                while self._doc.contents:\n",
    "                    child = self._doc.body.contents.pop(0)\n",
    "                    print(\"Moving child out: \", child)\n",
    "                    topCandidate.append(child)\n",
    "\n",
    "                self._doc.body.append(topCandidate)\n",
    "\n",
    "                self._initializeNode(topCandidate)\n",
    "            elif topCandidate:\n",
    "                # Find a better top candidate node if it contains (at least three) nodes which belong to `topCandidates` array\n",
    "                # and whose scores are quite closed with current `topCandidate` node.\n",
    "                alternativeCandidateAncestors = []\n",
    "                for i in range(1, len(topCandidates)):\n",
    "                    if (topCandidates[i].readability[\"contentScore\"] / topCandidate.readability[\"contentScore\"]) >= 0.75:\n",
    "                        alternativeCandidateAncestors.append(self._getNodeAncestors(topCandidates[i]))\n",
    "\n",
    "                MINIMUM_TOPCANDIDATES = 3\n",
    "                if len(alternativeCandidateAncestors) >= MINIMUM_TOPCANDIDATES:\n",
    "                    parentOfTopCandidate = topCandidate.parent\n",
    "                    while parentOfTopCandidate.name != \"body\":\n",
    "                        listsContainingThisAncestor = 0\n",
    "                        for ancestorIndex in range(len(alternativeCandidateAncestors)):\n",
    "                            listsContainingThisAncestor += int(parentOfTopCandidate in alternativeCandidateAncestors[ancestorIndex])\n",
    "                        if listsContainingThisAncestor >= MINIMUM_TOPCANDIDATES:\n",
    "                            topCandidate = parentOfTopCandidate\n",
    "                            break\n",
    "                        parentOfTopCandidate = parentOfTopCandidate.parent\n",
    "\n",
    "                if not topCandidate.readability:\n",
    "                    self._initializeNode(topCandidate)\n",
    "                \n",
    "                # Because of our bonus system, parents of candidates might have scores\n",
    "                # themselves. They get half of the node. There won't be nodes with higher\n",
    "                # scores than our topCandidate, but if we see the score going *up* in the first\n",
    "                # few steps up the tree, that's a decent sign that there might be more content\n",
    "                # lurking in other places that we want to unify in. The sibling stuff\n",
    "                # below does some of that - but only if we've looked high enough up the DOM\n",
    "                # tree.\n",
    "                parentOfTopCandidate = topCandidate.parent\n",
    "                lastScore = topCandidate.readability[\"contentScore\"]\n",
    "                # The scores shouldn't get too low\n",
    "                scoreThreshold = lastScore / 3\n",
    "                while parentOfTopCandidate.name != \"body\":\n",
    "                    if parentOfTopCandidate.readability:\n",
    "                        parentOfTopCandidate = parentOfTopCandidate.parent\n",
    "                        continue\n",
    "\n",
    "                    parentScore = parentOfTopCandidate.readability[\"contentScore\"]\n",
    "                    if parentScore < scoreThreshold:\n",
    "                        break\n",
    "                    if parentScore > lastScore:\n",
    "                        # Alright! We found a better parent to use.\n",
    "                        topCandidate = parentOfTopCandidate\n",
    "                        break\n",
    "                    \n",
    "                    lastScore = parentOfTopCandidate.readability[\"contentScore\"]\n",
    "                    parentOfTopCandidate = parentOfTopCandidate.parent\n",
    "                \n",
    "                # If the top candidate is the only child, use parent instead. This will help sibling\n",
    "                # joining logic when adjacent content is actually located in parent's sibling node.\n",
    "                parentOfTopCandidate = topCandidate.parent\n",
    "                while parentOfTopCandidate.name != \"body\" and len(parentOfTopCandidate.contents) == 1:\n",
    "                    topCandidate = parentOfTopCandidate\n",
    "                    parentOfTopCandidate = topCandidate.parent\n",
    "                \n",
    "                if not topCandidate.readability:\n",
    "                    self._initializeNode(topCandidate)\n",
    "            \n",
    "            # Now that we have the top candidate, look through its siblings for content\n",
    "            # that might also be related. Things like preambles, content split by ads\n",
    "            # that we removed, etc.\n",
    "            articleContent = self._doc.new_tag(\"div\")\n",
    "            if isPaging:\n",
    "                articleContent[\"id\"] = \"readability-content\"\n",
    "            \n",
    "            siblingScoreThreshold = max(10, topCandidate.readability[\"contentScore\"] * 0.2)\n",
    "            # Keep potential top candidate's parent node to try to get text direction of it later\n",
    "            parentOfTopCandidate = topCandidate.parent\n",
    "            siblings = parentOfTopCandidate.children\n",
    "\n",
    "            for sibling in siblings:\n",
    "                append = False\n",
    "\n",
    "                print(\"Looking at sibling node: \", sibling, sibling.readability.content_score if sibling.readability else \"\")\n",
    "                print(\"Sibling has score \", sibling.readability.content_score if sibling.readability else \"Unknown\")\n",
    "\n",
    "                if sibling == topCandidate:\n",
    "                    append = True\n",
    "                else:\n",
    "                    contentBonus = 0\n",
    "\n",
    "                # Give a bonus if sibling nodes and top candidates have the example same classname\n",
    "                if sibling.get(\"class\") == topCandidate.get(\"class\") and topCandidate.get(\"class\"):\n",
    "                    contentBonus += topCandidate.readability[\"contentScore\"] * 0.2\n",
    "\n",
    "                if sibling.readability and ((sibling.readability[\"contentScore\"] + contentBonus) >= siblingScoreThreshold):\n",
    "                    append = True\n",
    "                elif sibling.name == \"p\":\n",
    "                    linkDensity = self._getLinkDensity(sibling)\n",
    "                    nodeContent = self._getInnerText(sibling)\n",
    "                    nodeLength = len(nodeContent)\n",
    "\n",
    "                    if (nodeLength > 80 and linkDensity < 0.25):\n",
    "                        appent = True\n",
    "                    elif (nodeLength < 80 and nodeLength > 0 and linkDensity == 0 and re.search(r'\\.( |$)', nodeContent)):\n",
    "                        append = True\n",
    "                \n",
    "                if append:\n",
    "                    print(\"Appending node: \", sibling)\n",
    "                    if (sibling.name in self.ALTER_TO_DIV_EXCEPTIONS):\n",
    "                        # We have a node that isn't a common block level element, like a form or td tag.\n",
    "                        # Turn it into a div so it doesn't get filtered out later by accident.\n",
    "                        print(\"Altering sibling: \", sibling, \" to div.\")\n",
    "                        sibling = self._setNodeTag(sibling, \"div\")\n",
    "\n",
    "                    articleContent.append(sibling)\n",
    "                    # Fetch children again to make it compatible\n",
    "                    # with DOM parsers without live collection support\n",
    "                    siblings = parentOfTopCandidate.children\n",
    "                    # todo - there might be errors here. The original code is:\n",
    "                    #   // siblings is a reference to the children array, and\n",
    "                    #   // sibling is removed from the array when we call appendChild().\n",
    "                    #   // As a result, we must revisit this index since the nodes\n",
    "                    #   // have been shifted.\n",
    "                    #   s -= 1;\n",
    "                    #   sl -= 1;\n",
    "            # We have all the content that we need. Now we clean it up for presentation\n",
    "            print(\"Article content pre-prep: \" + articleContent)\n",
    "            self._prepArticle(articleContent)\n",
    "            print(\"Article content post-prep: \", articleContent)\n",
    "\n",
    "            if neededToCreateTopCandidate:\n",
    "                # We already create a fake div thing, and there wouldn't have been any siblings left\n",
    "                # for the previous loop, so there's no point trying to create a new div, and then\n",
    "                # move all the children over. Just assign IDs and class names here. No need to append\n",
    "                # because that already happened anyway\n",
    "                topCandidate.attrs[\"id\"] = [\"readability-page-1\"] # todo - this might need to be a list\n",
    "                topCandidate.attrs[\"class\"] = [\"page\"]\n",
    "            else:\n",
    "                div = self._doc.new_tag(\"div\")\n",
    "                div.attrs[\"id\"] = [\"readability-page-1\"]\n",
    "                div.attrs[\"class\"] = [\"page\"]\n",
    "                while articleContent.find():\n",
    "                    div.append(articleContent.pop(0))\n",
    "                articleContent.append(div)\n",
    "            \n",
    "            print(\"Article content after paging: \", articleContent)\n",
    "            parseSuccesful = True\n",
    "\n",
    "            # // Now that we've gone through the full algorithm, check to see if\n",
    "            # // we got any meaningful content. If we didn't, we may need to re-run\n",
    "            # // grabArticle with different flags set. This gives us a higher likelihood of\n",
    "            # // finding the content, and the sieve approach gives us a higher likelihood of\n",
    "            # // finding the -right- content.\n",
    "            textLength = len(self._getInnerText(articleContent, True))\n",
    "            if textLength < self._charThreshold:\n",
    "                parseSuccesful = False\n",
    "                page = pageCacheHtml\n",
    "\n",
    "                if self._flagIsActive(self.FLAG_STRIP_UNLIKELYS):\n",
    "                    self._removeFlag(self.FLAG_STRIP_UNLIKELYS)\n",
    "                    self._attempts.append({\"articleContent\": articleContent, \"textLength\": textLength})\n",
    "                elif self._flagIsActive(self.FLAG_WEIGHT_CLASSES):\n",
    "                    self.removeFlag(self.FLAG_WEIGHT_CLASSES)\n",
    "                    self._attempts.append({\"articleContent\": articleContent, \"textLength\": textLength})\n",
    "                elif self._flagIsActive(self.FLAG_CLEAN_CONDITIONALLY):\n",
    "                    self.removeFlag(self.FLAG_CLEAN_CONDITIONALLY)\n",
    "                    self._attempts.append({\"articleContent\": articleContent, \"textLength\": textLength})\n",
    "                else:\n",
    "                    self._attempts.append({\"articleContent\": articleContent, \"textLength\": textLength})\n",
    "                    # No luck after removing flags, just return the longest text we found during the different loops\n",
    "                    self.attempts.sort(key=lambda x: x[\"textLength\"], reverse=True)\n",
    "                    \n",
    "                    if self.attempts[0].get(\"textLength\", None) is None:\n",
    "                        return None\n",
    "                    \n",
    "                    articleContent = self._attempts[0][\"articleContent\"]\n",
    "                    parseSuccesful = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _postProcessContent(self, articleContent):\n",
    "        \"\"\"\n",
    "        /**\n",
    "        * Run any post-process modifications to article content as necessary.\n",
    "        *\n",
    "        * @param Element\n",
    "        * @return void\n",
    "        **/\n",
    "        \"\"\"\n",
    "        # Readability cannot open relative uris so we convert them to absolute uris.\n",
    "        articleContent = self._fixRelativeUris(articleContent)\n",
    "        articleContent = self._simplifyNestedElements(articleContent)\n",
    "        if not self._keepClasses:\n",
    "            # Remove classes.\n",
    "            articleConent = self._cleanClasses(articleContent)\n",
    "        return articleConent\n",
    "\n",
    "    def _removeNodes(self, nodeList, filterFn = None):\n",
    "        \"\"\"\n",
    "        /**\n",
    "        * Iterates over a NodeList, calls `filterFn` for each node and removes node\n",
    "        * if function returned `true`.\n",
    "        *\n",
    "        * If function is not passed, removes all the nodes in node list.\n",
    "        *\n",
    "        * @param NodeList nodeList The nodes to operate on\n",
    "        * @param Function filterFn the function to use as a filter\n",
    "        * @return void\n",
    "        */\n",
    "        \"\"\"\n",
    "        # Avoid ever operating on live node lists.\n",
    "        # if (self._docJSDOMParser and nodeList[\"_isLiveNodeList\"])\"\"\n",
    "            # raise Exception(\"Do not pass live node lists to _removeNodes\")\n",
    "        \n",
    "        for i in range(len(nodeList) - 1, 0, -1):\n",
    "            node = nodeList[i]\n",
    "            parentNode = node.parent\n",
    "            if parentNode:\n",
    "                if not filterFn or filterFn(node, i, nodeList):\n",
    "                    node.extract()\n",
    "\n",
    "    def _replaceNodeTags(self, nodeList, newTagName):\n",
    "        \"\"\"\n",
    "        /**\n",
    "        * Iterates over a NodeList, and calls _setNodeTag for each node.\n",
    "        *\n",
    "        * @param NodeList nodeList The nodes to operate on\n",
    "        * @param String newTagName the new tag name to use\n",
    "        * @return void\n",
    "        */\n",
    "        \"\"\"\n",
    "        # Avoid ever operating on live node lists.\n",
    "        # if (self._docJSDOMParser and nodeList[\"_isLiveNodeList\"]):\n",
    "            # raise Exception(\"Do not pass live node lists to _replaceNodeTags\")\n",
    "        \n",
    "        for node in nodeList:\n",
    "            self._setNodeTag(node, newTagName) # todo\n",
    "  \n",
    "    def _forEachNode(self, nodeList, fn):\n",
    "        \"\"\"\n",
    "        /**\n",
    "        * Iterate over a NodeList, which doesn't natively fully implement the Array\n",
    "        * interface.\n",
    "        *\n",
    "        * For convenience, the current object context is applied to the provided\n",
    "        * iterate function.\n",
    "        *\n",
    "        * @param  NodeList nodeList The NodeList.\n",
    "        * @param  Function fn       The iterate function.\n",
    "        * @return void\n",
    "        */\n",
    "        \"\"\"\n",
    "        for n in nodeList:\n",
    "            fn(n) # todo - maybe should be n = fn(n)\n",
    "\n",
    "    def _findNode(self, nodeList, fn):\n",
    "        \"\"\"\n",
    "        /**\n",
    "        * Iterate over a NodeList, and return the first node that passes\n",
    "        * the supplied test function\n",
    "        *\n",
    "        * For convenience, the current object context is applied to the provided\n",
    "        * test function.\n",
    "        *\n",
    "        * @param  NodeList nodeList The NodeList.\n",
    "        * @param  Function fn       The test function.\n",
    "        * @return void\n",
    "        */\"\"\"\n",
    "        for n in nodeList:\n",
    "            if fn(n):\n",
    "                return n\n",
    "            \n",
    "    def _someNode(self, nodeList, fn):\n",
    "        \"\"\"\n",
    "        /**\n",
    "        * Iterate over a NodeList, return true if any of the provided iterate\n",
    "        * function calls returns true, false otherwise.\n",
    "        *\n",
    "        * For convenience, the current object context is applied to the\n",
    "        * provided iterate function.\n",
    "        *\n",
    "        * @param  NodeList nodeList The NodeList.\n",
    "        * @param  Function fn       The iterate function.\n",
    "        * @return Boolean\n",
    "        */\n",
    "        \"\"\"\n",
    "        for n in nodeList:\n",
    "            if fn(n):\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "    def _everyNode(self, nodeList, fn):\n",
    "        \"\"\"\n",
    "        /**\n",
    "        * Iterate over a NodeList, return true if all of the provided iterate\n",
    "        * function calls return true, false otherwise.\n",
    "        *\n",
    "        * For convenience, the current object context is applied to the\n",
    "        * provided iterate function.\n",
    "        *\n",
    "        * @param  NodeList nodeList The NodeList.\n",
    "        * @param  Function fn       The iterate function.\n",
    "        * @return Boolean\n",
    "        */\n",
    "        \"\"\"\n",
    "        if [fn(n) for n in nodeList].all():\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def _getAllNodesWithTag(self, node, tagNames):\n",
    "        if node.select:\n",
    "            selector = \",\".join(tagNames)\n",
    "            return node.select(selector)\n",
    "        else:\n",
    "            nodes = []\n",
    "            for tag in tagNames:\n",
    "                collection = node.find_all(tag)\n",
    "                if isinstance(collection, list):\n",
    "                    nodes.extend(collection)\n",
    "                else:\n",
    "                    nodes.append(collection)\n",
    "        return nodes\n",
    "\n",
    "    def _unescapeHtmlEntities(self, str):\n",
    "        \"\"\"\n",
    "        /**\n",
    "        * Converts some of the common HTML entities in string to their corresponding characters.\n",
    "        *\n",
    "        * @param str {string} - a string to unescape.\n",
    "        * @return string without HTML entity.\n",
    "        */\n",
    "        \"\"\"\n",
    "        if not str:\n",
    "            return str\n",
    "\n",
    "        str = unescape(str)\n",
    "        str = re.sub(r\"&(#?[\\w\\d]+);\", lambda match: BeautifulSoup(match.group(0), \"html.parser\").text, str)\n",
    "        return str\n",
    " \n",
    "    def _getArticleMetadata(self, jsonld):\n",
    "        \"\"\"\n",
    "        /**\n",
    "        * Attempts to get excerpt and byline metadata for the article.\n",
    "        *\n",
    "        * @param {Object} jsonld — object containing any metadata that\n",
    "        * could be extracted from JSON-LD object.\n",
    "        *\n",
    "        * @return Object with optional \"excerpt\" and \"byline\" properties\n",
    "        */\n",
    "        \"\"\"\n",
    "        metadata = {}\n",
    "        values = {}\n",
    "        metaElements = self._doc.find_all(\"meta\")\n",
    "\n",
    "        # property is a space-separated list of values\n",
    "        propertyPattern = re.compile(r'\\s*(dc|dcterm|og|twitter)\\s*:\\s*(author|creator|description|title|site_name)\\s*', re.I)\n",
    "        \n",
    "        # name is a single value\n",
    "        namePattern = re.compile(r'^\\s*(?:(dc|dcterm|og|twitter|weibo:(article|webpage))\\s*[\\.:]\\s*)?(author|creator|description|title|site_name)\\s*$', re.I)\n",
    "\n",
    "        # Find description tags\n",
    "        for element in metaElements:\n",
    "            elementName = element.get('name')\n",
    "            elementProperty = element.get('property')\n",
    "            content = element.get('content')\n",
    "            if not content:\n",
    "                continue\n",
    "            matches = None\n",
    "            name = None\n",
    "            if elementProperty:\n",
    "                matches = propertyPattern.match(elementProperty)\n",
    "                if matches:\n",
    "                    # Convert to lowercase, and remove any whitespace\n",
    "                    # so we can match below.\n",
    "                    name = matches.group(0).lower().replace(' ', '')\n",
    "                    # multiple authors\n",
    "                    values[name] = content.strip()\n",
    "            if not matches and elementName and namePattern.match(elementName):\n",
    "                name = elementName\n",
    "                if content:\n",
    "                    # Convert to lowercase, remove any whitespace, and convert dots\n",
    "                    # to colons so we can match below.\n",
    "                    name = name.lower().replace(' ', '').replace('.', ':')\n",
    "                    values[name] = content.strip()\n",
    "\n",
    "        # get title\n",
    "        metadata['title'] = jsonld.get('title') or \\\n",
    "                            values.get('dc:title') or \\\n",
    "                            values.get('dcterm:title') or \\\n",
    "                            values.get('og:title') or \\\n",
    "                            values.get('weibo:article:title') or \\\n",
    "                            values.get('weibo:webpage:title') or \\\n",
    "                            values.get('title') or \\\n",
    "                            values.get('twitter:title')\n",
    "        if not metadata['title']:\n",
    "            metadata['title'] = self._getArticleTitle()\n",
    "\n",
    "        # get author\n",
    "        metadata['byline'] = jsonld.get('byline') or \\\n",
    "                            values.get('dc:creator') or \\\n",
    "                            values.get('dcterm:creator') or \\\n",
    "                            values.get('author')\n",
    "\n",
    "        # get description\n",
    "        metadata['excerpt'] = jsonld.get('excerpt') or \\\n",
    "                            values.get('dc:description') or \\\n",
    "                            values.get('dcterm:description') or \\\n",
    "                            values.get('og:description') or \\\n",
    "                            values.get('weibo:article:description') or \\\n",
    "                            values.get('weibo:webpage:description') or \\\n",
    "                            values.get('description') or \\\n",
    "                            values.get('twitter:description')\n",
    "\n",
    "        # get site name\n",
    "        metadata['siteName'] = jsonld.get('siteName') or \\\n",
    "                                values.get('og:site_name')\n",
    "\n",
    "        # in many sites the meta value is escaped with HTML entities,\n",
    "        # so here we need to unescape it\n",
    "        metadata['title'] = self._unescapeHtmlEntities(metadata['title'])\n",
    "        metadata['byline'] = self._unescapeHtmlEntities(metadata['byline'])\n",
    "        metadata['excerpt'] = self._unescapeHtmlEntities(metadata['excerpt'])\n",
    "        metadata['siteName'] = self._unescapeHtmlEntities(metadata['siteName'])\n",
    "\n",
    "        return metadata\n",
    "        \n",
    "    def _unwrapNoscriptImages(self, doc):\n",
    "        \"\"\"\n",
    "        /**\n",
    "        * Find all <noscript> that are located after <img> nodes, and which contain only one\n",
    "        * <img> element. Replace the first image with the image from inside the <noscript> tag,\n",
    "        * and remove the <noscript> tag. This improves the quality of the images we use on\n",
    "        * some sites (e.g. Medium).\n",
    "        *\n",
    "        * @param Element\n",
    "        **/\n",
    "        \"\"\"\n",
    "        # Find img without source or attributes that might contains image, and remove it.\n",
    "        # This is done to prevent a placeholder img is replaced by img from noscript in next step.\n",
    "        imgs = doc.find_all('img')\n",
    "        for img in imgs:\n",
    "            for attr in img.attrs:\n",
    "                if attr in ['src', 'srcset', 'data-src', 'data-srcset']:\n",
    "                    break\n",
    "                if re.search(r'\\.(jpg|jpeg|png|webp)$', attr):\n",
    "                    break\n",
    "            else:\n",
    "                img.extract()\n",
    "\n",
    "        # Next find noscript and try to extract its image\n",
    "        noscripts = doc.find_all('noscript')\n",
    "        for noscript in noscripts:\n",
    "            # Parse content of noscript and make sure it only contains image\n",
    "            # tmp = BeautifulSoup(noscript.contents[0], 'html.parser')\n",
    "            if not len(noscript.find_all('img')) == 1:\n",
    "                continue\n",
    "\n",
    "            # If noscript has previous sibling and it only contains image,\n",
    "            # replace it with noscript content. However we also keep old\n",
    "            # attributes that might contains image.\n",
    "            prevElement = noscript.previous_sibling\n",
    "            if prevElement and len(prevElement.find_all('img')) == 1:\n",
    "                prevImg = prevElement.find('img')\n",
    "                newImg = noscript.find('img')\n",
    "                for attr, value in prevImg.attrs.items():\n",
    "                    if value == \"\":\n",
    "                        continue\n",
    "                    if attr in ['src', 'srcset'] or re.search(r'\\.(jpg|jpeg|png|webp)$', value):\n",
    "                        if newImg.get(attr) == value:\n",
    "                            continue\n",
    "                        attrName = attr\n",
    "                        if newImg.has_attr(attrName):\n",
    "                            attrName = 'data-old-' + attrName\n",
    "                        newImg[attrName] = value\n",
    "                prevElement.replace_with(noscript.contents[0])\n",
    "\n",
    "    def _removeScripts(doc):\n",
    "        \"\"\"\n",
    "        /**\n",
    "        * Removes script tags from the document.\n",
    "        *\n",
    "        * @param Element\n",
    "        **/\n",
    "        \"\"\"\n",
    "        for script in doc.find_all(['script', 'noscript']):\n",
    "            script.extract()\n",
    "\n",
    "    def _isElementWithoutContent(self, node: element.Tag):\n",
    "        return (type(node) == element.Tag\n",
    "                and len(node.get_text().strip()) == 0\n",
    "                and (len(node.children) == 0\n",
    "                    or len(node.children) == (len(node.find_all(\"br\")) + len(node.find_all(\"hr\")))))\n",
    "\n",
    "    def _hasChildBlockElement(self, element):\n",
    "        \"\"\"\n",
    "        /**\n",
    "        * Determine whether element has any children block level elements.\n",
    "        *\n",
    "        * @param Element\n",
    "        */\n",
    "        \"\"\"\n",
    "        return any(\n",
    "            [node.name in self.DIV_TO_P_ELEMS or self._hasChildBlockElement(node) for node in element.contents]\n",
    "        )\n",
    "\n",
    "    def _isPhrasingContent(self, node: element.Tag) -> bool:\n",
    "        \"\"\"\n",
    "        /***\n",
    "        * Determine if a node qualifies as phrasing content.\n",
    "        * https://developer.mozilla.org/en-US/docs/Web/Guide/HTML/Content_categories#Phrasing_content\n",
    "        **/\n",
    "        \"\"\"\n",
    "        # if node.name is None:  # node is a NavigableString\n",
    "        if type(node) == element.NavigableString:\n",
    "            return True\n",
    "        if node.name.upper() in self.PHRASING_ELEMS:\n",
    "            return True\n",
    "        if node.name in [\"a\", \"del\", \"ins\"]:\n",
    "            return all(self._isPhrasingContent(child) for child in node.contents)\n",
    "        return False\n",
    "\n",
    "    def _isWhitespace(self, node: element.Tag) -> bool:\n",
    "        if type(node) == element.NavigableString:\n",
    "            return node.strip() == \"\"\n",
    "        if node.name == \"br\":\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _getInnerText(self, e: element.Tag, normalizeSpaces: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        /**\n",
    "        * Get the inner text of a node - cross browser compatibly.\n",
    "        * This also strips out any excess whitespace to be found.\n",
    "        *\n",
    "        * @param Element\n",
    "        * @param Boolean normalizeSpaces (default: true)\n",
    "        * @return string\n",
    "        **/\n",
    "        \"\"\"\n",
    "        textContent = e.get_text().strip()\n",
    "\n",
    "        if normalizeSpaces:\n",
    "            return self.REGEXPS(\"normalize\").sub(\" \", textContent)\n",
    "        return textContent\n",
    "\n",
    "    def _getCharCount(self, e: element.Tag, s: str = \",\") -> int:\n",
    "        \"\"\"\n",
    "        /**\n",
    "        * Get the number of times a string s appears in the node e.\n",
    "        *\n",
    "        * @param Element\n",
    "        * @param string - what to split on. Default is \",\"\n",
    "        * @return number (integer)\n",
    "        **/\n",
    "        \"\"\"\n",
    "        return len(self._getInnerText(e).split(s)) - 1\n",
    "    \n",
    "    def _cleanStyles(self, e: element.Tag) -> None:\n",
    "        if not e or e.name == \"svg\":\n",
    "            return\n",
    "        \n",
    "        # Remove `style` and deprecated presentational attributes\n",
    "        for attr in self.PRESENTATIONAL_ATTRIBUTES:\n",
    "            e.attrs.pop(attr, None)\n",
    "        \n",
    "        if e.name.upper() in self.DEPRECATED_SIZE_ATTRIBUTE_ELEMS:\n",
    "            e.attr.pop(\"width\")\n",
    "            e.attr.pop(\"height\")\n",
    "        \n",
    "        for child in e.children:\n",
    "            if isinstance(child, element.Tag):\n",
    "                self._cleanStyles(child)\n",
    "\n",
    "    def _getLinkDensity(self, element: element.Tag) -> Union[int, float]:\n",
    "        \"\"\"\n",
    "        /**\n",
    "        * Get the density of links as a percentage of the content\n",
    "        * This is the amount of text that is inside a link divided by the total text in the node.\n",
    "        *\n",
    "        * @param Element\n",
    "        * @return number (float)\n",
    "        **/\n",
    "        \"\"\"\n",
    "        textLength = len(self._getInnerText(element))\n",
    "        if textLength == 0:\n",
    "            return 0\n",
    "        \n",
    "        linkLength = 0\n",
    "        for linkNode in element.select(\"a\"):\n",
    "            href = linkNode.get(\"href\")\n",
    "            coefficient = 0.3 if href and self.REGEXPS[\"hashUrl\"].match(href) else 1\n",
    "            linkLength += len(self._getInnerText(linkNode)) * coefficient\n",
    "        \n",
    "        return linkLength / textLength\n",
    "    \n",
    "    def _getClassWeight(self, e: element.Tag) -> int:\n",
    "        \"\"\"\n",
    "        /**\n",
    "        * Get an elements class/id weight. Uses regular expressions to tell if this\n",
    "        * element looks good or bad.\n",
    "        *\n",
    "        * @param Element\n",
    "        * @return number (Integer)\n",
    "        **/\n",
    "        \"\"\"\n",
    "        if not self.flagIsActive(self.FLAG_WEIGHT_CLASSES):\n",
    "            return 0\n",
    "        \n",
    "        weight = 0\n",
    "\n",
    "        # Look for a special classname\n",
    "        class_name = e.get(\"class\")\n",
    "        if class_name and isinstance(class_name, list):\n",
    "            class_name = \" \".join(class_name)\n",
    "            if self.REGEXPS[\"negative\"].search(class_name):\n",
    "                weight -= 25\n",
    "            if self.REGEXPS[\"positive\"].search(class_name):\n",
    "                weight += 25\n",
    "        \n",
    "        # Look for a special ID\n",
    "        element_id = e.get(\"id\")\n",
    "        if element_id:\n",
    "            if self.REGEXPS[\"negative\"].search(element_id):\n",
    "                weight -= 25\n",
    "            if self.REGEXPS[\"positive\"].search(element_id):\n",
    "                weight += 25\n",
    "\n",
    "        return weight\n",
    "\n",
    "    def _getArticleTitle(self):\n",
    "        curTitle = \"\"\n",
    "        origTitle = \"\"\n",
    "\n",
    "        try:\n",
    "            curTitle = origTitle = self._doc.title.string.strip()\n",
    "\n",
    "            # If they had an element with id \"title\" in their HTML\n",
    "            if not isinstance(curTitle, str):\n",
    "                curTitle = origTitle = self._getInnerText(self._doc.find(\"title\"))\n",
    "        except:\n",
    "            # ignore exceptions setting the title.\n",
    "            pass\n",
    "\n",
    "        titleHadHierarchicalSeparators = False\n",
    "\n",
    "        def wordCount(str):\n",
    "            return len(str.split())\n",
    "\n",
    "        # If there's a separator in the title, first remove the final part\n",
    "        if re.search(r' [\\|\\-\\\\\\/>»] ', curTitle):\n",
    "            titleHadHierarchicalSeparators = re.search(r' [\\\\\\/>»] ', curTitle)\n",
    "            curTitle = re.sub(r'(.*)[\\|\\-\\\\\\/>»] .*', r'\\1', origTitle)\n",
    "\n",
    "            # If the resulting title is too short (3 words or fewer), remove\n",
    "            # the first part instead:\n",
    "            if wordCount(curTitle) < 3:\n",
    "                curTitle = re.sub(r'[^\\|\\-\\\\\\/>»]*[\\|\\-\\\\\\/>»](.*)', r'\\1', origTitle)\n",
    "            elif curTitle.find(\": \") != -1:\n",
    "                # Check if we have an heading containing this exact string, so we\n",
    "                # could assume it's the full title.\n",
    "                headings = self._concatNodeLists(\n",
    "                    self._doc.find_all([\"h1\", \"h2\"])\n",
    "                )\n",
    "                trimmedTitle = curTitle.strip()\n",
    "                match = any([heading.get_text().strip() == trimmedTitle for heading in headings])\n",
    "\n",
    "                # If we don't, let's extract the title out of the original title string.\n",
    "                if not match:\n",
    "                    curTitle = origTitle[origTitle.rfind(\":\") + 1:]\n",
    "\n",
    "                    # If the title is now too short, try the first colon instead:\n",
    "                    if wordCount(curTitle) < 3:\n",
    "                        curTitle = origTitle[origTitle.find(\":\") + 1:]\n",
    "                        # But if we have too many words before the colon there's something weird\n",
    "                        # with the titles and the H tags so let's just use the original title instead\n",
    "                    elif wordCount(origTitle[0:origTitle.find(\":\")]) > 5:\n",
    "                        curTitle = origTitle\n",
    "            elif len(curTitle) > 150 or len(curTitle) < 15:\n",
    "                hOnes = self._doc.find_all(\"h1\")\n",
    "\n",
    "                if len(hOnes) == 1:\n",
    "                    curTitle = self._getInnerText(hOnes[0])\n",
    "\n",
    "            curTitle = re.sub(self.REGEXPS[\"normalize\"], \" \", curTitle.strip())\n",
    "            # If we now have 4 words or fewer as our title, and either no\n",
    "            # 'hierarchical' separators (\\, /, > or ») were found in the original\n",
    "            # title or we decreased the number of words by more than 1 word, use\n",
    "            # the original title.\n",
    "            curTitleWordCount = wordCount(curTitle)\n",
    "            if (curTitleWordCount <= 4 and\n",
    "                (not titleHadHierarchicalSeparators or\n",
    "                curTitleWordCount != wordCount(re.sub(r'[\\|\\-\\\\\\/>»]+', '', origTitle)) - 1)):\n",
    "                curTitle = origTitle\n",
    "            return curTitle\n",
    "\n",
    "    def _prepDocument(self):\n",
    "        \"\"\"\n",
    "        /**\n",
    "        * Prepare the HTML document for readability to scrape it.\n",
    "        * This includes things like stripping javascript, CSS, and handling terrible markup.\n",
    "        *\n",
    "        * @return void\n",
    "        **/\n",
    "        \"\"\"\n",
    "        doc = self._doc\n",
    "\n",
    "        # Remove all style tags in head\n",
    "        self._removeNodes(self._getAllNodesWithTag(doc, [\"style\"]))\n",
    "\n",
    "        if doc.body:\n",
    "            self._replaceBrs(doc.body)\n",
    "\n",
    "        self._replaceNodeTags(self._getAllNodesWithTag(doc, [\"font\"]), \"span\")\n",
    "\n",
    "    def _nextNode(self, node):\n",
    "        \"\"\"\n",
    "        /**\n",
    "        * Finds the next node, starting from the given node, and ignoring\n",
    "        * whitespace in between. If the given node is an element, the same node is\n",
    "        * returned.\n",
    "        */\n",
    "        \"\"\"\n",
    "        next_node = node\n",
    "        while next_node and (type(next_node) == element.Tag) and self.REGEXPS['whitespace'].search(next_node.textContent):\n",
    "            next_node = next_node.nextSibling\n",
    "        return next_node\n",
    "\n",
    "    def _replaceBrs(self, elem):\n",
    "        \"\"\"\n",
    "        /**\n",
    "        * Replaces 2 or more successive <br> elements with a single <p>.\n",
    "        * Whitespace between <br> elements are ignored. For example:\n",
    "        *   <div>foo<br>bar<br> <br><br>abc</div>\n",
    "        * will become:\n",
    "        *   <div>foo<br>bar<p>abc</p></div>\n",
    "        */\n",
    "        \"\"\"\n",
    "        brs = elem.find_all('br')\n",
    "        for br in brs:\n",
    "            next = br.next_sibling\n",
    "\n",
    "            # Whether 2 or more <br> elements have been found and replaced with a\n",
    "            # <p> block.\n",
    "            replaced = False\n",
    "\n",
    "            # If we find a <br> chain, remove the <br>s until we hit another node\n",
    "            # or non-whitespace. This leaves behind the first <br> in the chain\n",
    "            # (which will be replaced with a <p> later).\n",
    "            while next and next.name == 'br':\n",
    "                replaced = True\n",
    "                br_sibling = next.next_sibling\n",
    "                next.extract()\n",
    "                next = br_sibling\n",
    "\n",
    "            # If we removed a <br> chain, replace the remaining <br> with a <p>. Add\n",
    "            # all sibling nodes as children of the <p> until we hit another <br>\n",
    "            # chain.\n",
    "            if replaced:\n",
    "                p = self._doc.new_tag(\"p\")\n",
    "                br.replace_with(p)\n",
    "\n",
    "                next = p.next_sibling\n",
    "                while next:\n",
    "                    # If we've hit another <br><br>, we're done adding children to this <p>.\n",
    "                    if next.name == 'br':\n",
    "                        next_elem = self._nextNode(next.next_sibling)\n",
    "                        if next_elem and next_elem.name == 'br':\n",
    "                            break\n",
    "\n",
    "                    if not self._isPhrasingContent(next):\n",
    "                        break\n",
    "\n",
    "                    # Otherwise, make this node a child of the new <p>.\n",
    "                    sibling = next.next_sibling\n",
    "                    p.append(next)\n",
    "                    next = sibling\n",
    "\n",
    "                while p.last_child and self._is_whitespace(p.last_child):\n",
    "                    p.last_child.extract()\n",
    "\n",
    "                if p.parent.name == \"p\":\n",
    "                    self._setNodeTag(p.parent, \"div\")\n",
    "\n",
    "    def _setNodeTag(self, node, tag):\n",
    "        print(\"_setNodeTag\", node, tag)\n",
    "\n",
    "        replacement = self._doc.new_tag(tag)\n",
    "        while node.contents:\n",
    "            replacement.append(node.contents[0])\n",
    "        node.replace_with(replacement)\n",
    "        \n",
    "        print(\"Will this line error?\")\n",
    "        if node.readability:\n",
    "            replacement.readability = node.readability\n",
    "\n",
    "        for attr, value in node.attrs.items():\n",
    "            try:\n",
    "                replacement[attr] = value\n",
    "            except Exception as ex:\n",
    "                \"\"\"\n",
    "                It's possible for setAttribute() to throw if the attribute name isn't a valid XML Name.\n",
    "                Such attributes can however be parsed from source in HTML docs, see\n",
    "                https://github.com/whatwg/html/issues/4275, so we can hit them here and then throw. We\n",
    "                don't care about such attributes so we ignore them.\n",
    "                \"\"\"\n",
    "                print(\"_setNodetag Error - \", ex)\n",
    "        return replacement\n",
    "\n",
    "    def _headerDuplicatesTitle(self, node):\n",
    "        \"\"\"\n",
    "        /**\n",
    "        * Check if this node is an H1 or H2 element whose content is mostly\n",
    "        * the same as the article title.\n",
    "        *\n",
    "        * @param Element  the node to check.\n",
    "        * @return boolean indicating whether this is a title-like header.\n",
    "        */\n",
    "        \"\"\"\n",
    "        if (node.name not in [\"h1\", \"h2\"]):\n",
    "            return False\n",
    "        heading = self._getInnerText(node, False)\n",
    "        print(\"Evaluating similarity of header: \", heading, self._articleTitle)\n",
    "        return self._textSimilarity(self._articleTitle, heading) > 0.75\n",
    "\n",
    "    def _flagIsActive(self, flag):\n",
    "        return (self._flags & flag) > 0\n",
    "    \n",
    "    def _removeFlag(self, flag):\n",
    "        self._flags = self._flags & ~flag\n",
    "\n",
    "    def _isProbablyVisible(self, node: element.Tag):\n",
    "        # Have to null-check node.style and node.className.indexOf to deal with SVG and MathML nodes.\n",
    "        return (\n",
    "            (not node.attrs.get(\"style\", None) or \"display: none\" not in node.attrs.get(\"style\", \"\"))\n",
    "            and not node.attrs.get(\"hidden\", None)\n",
    "            # check for \"fallback-image\" so that wikimedia math images are displayed\n",
    "            and ((\"aria-hidden\" not in node.attrs)\n",
    "                 or node.attrs.get(\"aria-hidden\", \"False\") != \"true\"\n",
    "                 or (\"fallback-image\" not in node.name))\n",
    "        )\n",
    "    \n",
    "    def parse(self):\n",
    "        # Avoid parsing too large documents, as per configuration option\n",
    "        if self._maxElemsToParse > 0:\n",
    "            numTags = len(self._doc.find_all())\n",
    "            if numTags > self._maxElemsToParse:\n",
    "                raise ValueError(f\"Aborting parsing document; {numTags} elements found\")\n",
    "\n",
    "        # Unwrap image from noscript\n",
    "        self._unwrapNoscriptImages(self._doc)\n",
    "\n",
    "        # Extract JSON-LD metadata before removing scripts\n",
    "        jsonLd = {} # if self._disableJSONLD else self._getJSONLD(self._doc)\n",
    "\n",
    "        # Remove script tags from the document.\n",
    "        # self._removeScripts(this._doc)\n",
    "        for script in self._doc([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "\n",
    "        self._prepDocument()\n",
    "\n",
    "        metadata = self._getArticleMetadata(jsonLd)\n",
    "        self._articleTitle = metadata[\"title\"]\n",
    "        print(self._articleTitle)\n",
    "\n",
    "        articleContent = self._grabArticle()\n",
    "        if not articleContent:\n",
    "            return None\n",
    "\n",
    "        # No fancy logging yet\n",
    "        print(f\"Grabbed: {articleContent.text}\")\n",
    "\n",
    "        self._postProcessContent(articleContent)\n",
    "\n",
    "        # If we haven't found an excerpt in the article's metadata, use the article's\n",
    "        # first paragraph as the excerpt. This is used for displaying a preview of\n",
    "        # the article's content.\n",
    "        if not metadata.excerpt:\n",
    "            paragraphs = articleContent.find_all(\"p\")\n",
    "            if paragraphs:\n",
    "                metadata.excerpt = paragraphs[0].text.strip()\n",
    "\n",
    "        textContent = articleContent.text\n",
    "        return {\n",
    "            \"title\": self._articleTitle,\n",
    "            \"byline\": metadata.byline or self._articleByline,\n",
    "            \"dir\": self._articleDir,\n",
    "            \"lang\": self._articleLang,\n",
    "            \"content\": str(articleContent),\n",
    "            \"textContent\": textContent,\n",
    "            \"length\": len(textContent),\n",
    "            \"excerpt\": metadata.excerpt,\n",
    "            \"siteName\": metadata.siteName or self._articleSiteName,\n",
    "        }\n",
    "        \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://www.builtinla.com/job/engineer/full-stack-software-engineer/132124\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Readability(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_setNodeTag <p><em><span><span><span><span><span><span><span><span><span>*VISA Sponsorship is NOT available for this position*</span></span></span></span></span></span></span></span></span></em><p><span><span>Salary will depend on various factors including applicant's prior relevant job experience, skill set, and geographic location. People Science offers a benefit package for full-time employees which includes: Medical, Dental, Vision, Flexible Spending Account, Life Insurance, DepCare FSA, Flexible Vacation Time Policy, Holidays, and Employee Stock Options. People Science reserves the right to amend, change, alter, and revise pay ranges and benefits offerings at any time. It is at the Company's discretion to determine what pay is provided to a candidate within the range associated with the role.</span></span></p></p> div\n",
      "Will this line error?\n",
      "Full-Stack Software Engineer (Greater LA Area, CA or Remote) - People Science\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_grabArticle() missing 1 required positional argument: 'page'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m reader\u001b[39m.\u001b[39;49mparse()\n",
      "Cell \u001b[0;32mIn[80], line 850\u001b[0m, in \u001b[0;36mReadability.parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_articleTitle \u001b[39m=\u001b[39m metadata[\u001b[39m\"\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    848\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_articleTitle)\n\u001b[0;32m--> 850\u001b[0m articleContent \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_grabArticle()\n\u001b[1;32m    851\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m articleContent:\n\u001b[1;32m    852\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: _grabArticle() missing 1 required positional argument: 'page'"
     ]
    }
   ],
   "source": [
    "reader.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
