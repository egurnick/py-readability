{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, element\n",
    "from html import unescape\n",
    "import re\n",
    "import requests\n",
    "\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {1: \"a\", 2: \"b\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0x6'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hex(0x2 |\n",
    "    0x4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = lambda x: (\n",
    "    x + 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Readability:\n",
    "    def __init__(self, doc, options={}):\n",
    "        self.doc = doc\n",
    "        # self.FLAG_STRIP_UNLIKELYS= \"0x1\" # hex\n",
    "        # self.FLAG_WEIGHT_CLASSES= \"0x2\" # hex\n",
    "        # self.FLAG_CLEAN_CONDITIONALLY= \"0x4\" # hex\n",
    "        self.FLAG_STRIP_UNLIKELYS = 0x1\n",
    "        self.FLAG_WEIGHT_CLASSES = 0x2\n",
    "        self.FLAG_CLEAN_CONDITIONALLY = 0x4\n",
    "\n",
    "        #   // https://developer.mozilla.org/en-US/docs/Web/API/Node/nodeType\n",
    "        self.ELEMENT_NODE= 1,\n",
    "        self.TEXT_NODE= 3,\n",
    "\n",
    "        #   // Max number of nodes supported by this parser. Default: 0 (no limit)\n",
    "        self.DEFAULT_MAX_ELEMS_TO_PARSE= 0\n",
    "\n",
    "        #   // The number of top candidates to consider when analysing how\n",
    "        #   // tight the competition is among candidates.\n",
    "        self.DEFAULT_N_TOP_CANDIDATES= 5\n",
    "\n",
    "        # // Element tags to score by default.\n",
    "        self.DEFAULT_TAGS_TO_SCORE= \"section,h2,h3,h4,h5,h6,p,td,pre\".upper().split(\",\"),\n",
    "\n",
    "        # // The default number of chars an article must have in order to return a result\n",
    "        self.DEFAULT_CHAR_THRESHOLD= 500\n",
    "\n",
    "        # // All of the regular expressions in use within readability.\n",
    "        # // Defined up here so we don't instantiate them repeatedly in loops.\n",
    "        self.REGEXPS = {\n",
    "            # NOTE: These two regular expressions are duplicated in\n",
    "            # Readability-readerable.js. Please keep both copies in sync.\n",
    "            'unlikelyCandidates': re.compile(r'-ad-|ai2html|banner|breadcrumbs|combx|comment|community|cover-wrap|disqus|extra|footer|gdpr|header|legends|menu|related|remark|replies|rss|shoutbox|sidebar|skyscraper|social|sponsor|supplemental|ad-break|agegate|pagination|pager|popup|yom-remote', re.IGNORECASE),\n",
    "            'okMaybeItsACandidate': re.compile(r'and|article|body|column|content|main|shadow', re.IGNORECASE),\n",
    "\n",
    "            'positive': re.compile(r'article|body|content|entry|hentry|h-entry|main|page|pagination|post|text|blog|story', re.IGNORECASE),\n",
    "            'negative': re.compile(r'-ad-|hidden|^hid$| hid$| hid |^hid |banner|combx|comment|com-|contact|foot|footer|footnote|gdpr|masthead|media|meta|outbrain|promo|related|scroll|share|shoutbox|sidebar|skyscraper|sponsor|shopping|tags|tool|widget', re.IGNORECASE),\n",
    "            'extraneous': re.compile(r'print|archive|comment|discuss|e[\\-]?mail|share|reply|all|login|sign|single|utility', re.IGNORECASE),\n",
    "            'byline': re.compile(r'byline|author|dateline|writtenby|p-author', re.IGNORECASE),\n",
    "            'replaceFonts': re.compile(r'<(\\/?)font[^>]*>', re.IGNORECASE),\n",
    "            'normalize': re.compile(r'\\s{2,}'),\n",
    "            'videos': re.compile(r'\\/\\/(www\\.)?((dailymotion|youtube|youtube-nocookie|player\\.vimeo|v\\.qq)\\.com|(archive|upload\\.wikimedia)\\.org|player\\.twitch\\.tv)', re.IGNORECASE),\n",
    "            'shareElements': re.compile(r'(\\b|_)(share|sharedaddy)(\\b|_)', re.IGNORECASE),\n",
    "            'nextLink': re.compile(r'(next|weiter|continue|>([^\\|]|$)|»([^\\|]|$))', re.IGNORECASE),\n",
    "            'prevLink': re.compile(r'(prev|earl|old|new|<|«)', re.IGNORECASE),\n",
    "            'tokenize': re.compile(r'\\W+'),\n",
    "            'whitespace': re.compile(r'^\\s*$'),\n",
    "            'hasContent': re.compile(r'\\S$'),\n",
    "            'hashUrl': re.compile(r'^#.+'),\n",
    "            'srcsetUrl': re.compile(r'(\\S+)(\\s+[\\d.]+[xw])?(\\s*(?:,|$))'),\n",
    "            'b64DataUrl': re.compile(r'^data:\\s*([^\\s;,]+)\\s*;\\s*base64\\s*,', re.IGNORECASE),\n",
    "            # See: https://schema.org/Article\n",
    "            'jsonLdArticleTypes': re.compile(r'^Article|AdvertiserContentArticle|NewsArticle|AnalysisNewsArticle|AskPublicNewsArticle|BackgroundNewsArticle|OpinionNewsArticle|ReportageNewsArticle|ReviewNewsArticle|Report|SatiricalArticle|ScholarlyArticle|MedicalScholarlyArticle|SocialMediaPosting|BlogPosting|LiveBlogPosting|DiscussionForumPosting|TechArticle|APIReference$'),\n",
    "        }\n",
    "\n",
    "        self.UNLIKELY_ROLES = [ \"menu\", \"menubar\", \"complementary\", \"navigation\", \"alert\", \"alertdialog\", \"dialog\" ]\n",
    "\n",
    "        self.DIV_TO_P_ELEMS =  {\"BLOCKQUOTE\", \"DL\", \"DIV\", \"IMG\", \"OL\", \"P\", \"PRE\", \"TABLE\", \"UL\"}\n",
    "\n",
    "        self.ALTER_TO_DIV_EXCEPTIONS = [\"DIV\", \"ARTICLE\", \"SECTION\", \"P\"]\n",
    "\n",
    "        self.PRESENTATIONAL_ATTRIBUTES = [ \"align\", \"background\", \"bgcolor\", \"border\", \"cellpadding\", \"cellspacing\", \"frame\", \"hspace\", \"rules\", \"style\", \"valign\", \"vspace\" ]\n",
    "\n",
    "        self.DEPRECATED_SIZE_ATTRIBUTE_ELEMS = [ \"TABLE\", \"TH\", \"TD\", \"HR\", \"PRE\" ]\n",
    "\n",
    "        # // The commented out elements qualify as phrasing content but tend to be\n",
    "        # // removed by readability when put into paragraphs, so we ignore them here.\n",
    "        self.PHRASING_ELEMS = [\n",
    "            # // \"CANVAS\", \"IFRAME\", \"SVG\", \"VIDEO\",\n",
    "            \"ABBR\", \"AUDIO\", \"B\", \"BDO\", \"BR\", \"BUTTON\", \"CITE\", \"CODE\", \"DATA\",\n",
    "            \"DATALIST\", \"DFN\", \"EM\", \"EMBED\", \"I\", \"IMG\", \"INPUT\", \"KBD\", \"LABEL\",\n",
    "            \"MARK\", \"MATH\", \"METER\", \"NOSCRIPT\", \"OBJECT\", \"OUTPUT\", \"PROGRESS\", \"Q\",\n",
    "            \"RUBY\", \"SAMP\", \"SCRIPT\", \"SELECT\", \"SMALL\", \"SPAN\", \"STRONG\", \"SUB\",\n",
    "            \"SUP\", \"TEXTAREA\", \"TIME\", \"VAR\", \"WBR\"\n",
    "        ]\n",
    "        \n",
    "\n",
    "        # // These are the classes that readability sets itself.\n",
    "        self.CLASSES_TO_PRESERVE = [ \"page\" ]\n",
    "\n",
    "        # // These are the list of HTML entities that need to be escaped.\n",
    "        self.HTML_ESCAPE_MAP = {\n",
    "            \"lt\": \"<\",\n",
    "            \"gt\": \">\",\n",
    "            \"amp\": \"&\",\n",
    "            \"quot\": '\"',\n",
    "            \"apos\": \"'\",\n",
    "        }\n",
    "\n",
    "        self._doc = BeautifulSoup(doc, \"html.parser\") # doc\n",
    "        # self._docJSDOMParser = self._doc.firstChild.__JSDOMParser__; # Needs work\n",
    "        self._articleTitle = None\n",
    "        self._articleByline = None\n",
    "        self._articleDir = None\n",
    "        self._articleSiteName = None\n",
    "        self._attempts = []\n",
    "\n",
    "        # // Configurable options\n",
    "        self._debug = options.get(\"debug\", True) # Does not originally have a default\n",
    "        \n",
    "    \n",
    "        self._maxElemsToParse = options.get(\"maxElemsToParse\", self.DEFAULT_MAX_ELEMS_TO_PARSE)\n",
    "        self._nbTopCandidates = options.get(\"nbTopCandidates\", self.DEFAULT_N_TOP_CANDIDATES)\n",
    "        self._charThreshold = options.get(\"charThreshold\", self.DEFAULT_CHAR_THRESHOLD)\n",
    "        self._classesToPreserve = self.CLASSES_TO_PRESERVE + options.get(\"classesToPreserve\", [])\n",
    "        self._keepClasses = options.get(\"keepClasses\", True) # Does not originally have a default\n",
    "        self.serializer = options.get(\"serializer\", lambda el: el[\"innterHTML\"])\n",
    "        self._disableJSONLD = options.get(\"disableJSONLD\", False) # Does not originally have a default\n",
    "\n",
    "        # // Start with all flags set\n",
    "        # self._flags = hex(self.FLAG_STRIP_UNLIKELYS |\n",
    "        #                 self.FLAG_WEIGHT_CLASSES |\n",
    "        #                 self.FLAG_CLEAN_CONDITIONALLY)\n",
    "        self._flags = self.FLAG_STRIP_UNLIKELYS | self.FLAG_WEIGHT_CLASSES | self.FLAG_CLEAN_CONDITIONALLY\n",
    "\n",
    "#     def Readability(self, doc = None, options = {}):\n",
    "#         \"\"\"\n",
    "#         Does not support passing a URI as the first argument\n",
    "# \n",
    "#         Arguments\n",
    "#             * options (dict): \n",
    "#                 * debug (bool)\n",
    "#                 * maxElemsToParse (int?) optional\n",
    "#                 * nbTopCandidates (?) optional\n",
    "#                 * charThreshold (int?) optional\n",
    "#                 * classesToPreserve (?) optional\n",
    "#                 * keepClasses (bool)\n",
    "#                 * serializer (func) optional\n",
    "#                 * disableJSONLD\n",
    "# \n",
    "#             The options object accepts a number of properties, all optional:\n",
    "# \n",
    "#             * debug (boolean, default false): whether to enable logging.\n",
    "#             * maxElemsToParse (number, default 0 i.e. no limit): the maximum number of elements to parse.\n",
    "#             * nbTopCandidates (number, default 5): the number of top candidates to consider when analysing how tight the competition is among candidates.\n",
    "#             * charThreshold (number, default 500): the number of characters an article must have in order to return a result.\n",
    "#             * classesToPreserve (array): a set of classes to preserve on HTML elements when the keepClasses options is set to false.\n",
    "#             * keepClasses (boolean, default false): whether to preserve all classes on HTML elements. When set to false only classes specified in the classesToPreserve array are kept.\n",
    "#             * disableJSONLD (boolean, default false): when extracting page metadata, Readability gives precendence to Schema.org fields specified in the JSON-LD format. Set this option to true to skip JSON-LD parsing.\n",
    "#             * serializer (function, default el => el.innerHTML) controls how the the content property returned by the parse() method is produced from the root DOM element. It may be useful to specify the serializer as the identity function (el => el) to obtain a DOM element instead of a string for content if you plan to process it further.\n",
    "# \n",
    "#         \"\"\"\n",
    "#         if (doc is None):\n",
    "#             raise Exception(\"First argument to Readability constructor should be a document object.\");\n",
    "#     \n",
    "#         # Bad code, fix later\n",
    "#         if \"documentElement\" not in doc:\n",
    "#             raise Exception(\"First argument to Readability constructor should be a document object.\");\n",
    "# \n",
    "#         self._doc = doc;\n",
    "#         self._docJSDOMParser = self._doc.firstChild.__JSDOMParser__; # Needs work\n",
    "#         self._articleTitle = None;\n",
    "#         self._articleByline = None;\n",
    "#         self._articleDir = None;\n",
    "#         self._articleSiteName = None;\n",
    "#         self._attempts = [];\n",
    "# \n",
    "#         # // Configurable options\n",
    "#         self._debug = options[\"debug\"];\n",
    "#         \n",
    "#     \n",
    "#         self._maxElemsToParse = options.get(\"maxElemsToParse\", self.DEFAULT_MAX_ELEMS_TO_PARSE)\n",
    "#         self._nbTopCandidates = options.get(\"nbTopCandidates\", self.DEFAULT_N_TOP_CANDIDATES)\n",
    "#         self._charThreshold = options.get(\"charThreshold\", self.DEFAULT_CHAR_THRESHOLD)\n",
    "#         self._classesToPreserve = this.CLASSES_TO_PRESERVE + options.get(\"classesToPreserve\", [])\n",
    "#         self._keepClasses = options[\"keepClasses\"];\n",
    "#         self.serializer = options.get(\"serializer\", lambda el: el[\"innterHTML\"])\n",
    "#         self._disableJSONLD = options[\"disableJSONLD\"];\n",
    "# \n",
    "#         # // Start with all flags set\n",
    "#         self._flags = hex(self.FLAG_STRIP_UNLIKELYS |\n",
    "#                         self.FLAG_WEIGHT_CLASSES |\n",
    "#                         self.FLAG_CLEAN_CONDITIONALLY)\n",
    "\n",
    "\n",
    "        # // Control whether log messages are sent to the console\n",
    "#         if (self._debug) {\n",
    "#             logNode = lambda node: (\n",
    "#                 if (node[\"nodeType\"] == node[\"TEXT_NODE\"]) {\n",
    "#                     return \"{0} (\\\"{1}\\\")\".format(node[\"nodeName\"], node[\"nodeContext\"])\n",
    "#                 }\n",
    "#                 \n",
    "#                 attributes = node.get(\"attributes\", [])\n",
    "#                 attrPairs = \" \".join([\"{0}=\\\"{1}\\\"\".format(attr[\"name\"], attr[\"value\"]), for attr in node.get(\"attributes\", [])])\n",
    "# \n",
    "#                 return \"<{0} {1}>\".format(node[\"localName\"], attrPairs)\n",
    "#             )\n",
    "# \n",
    "#             this.log = function () {\n",
    "#             if (typeof dump !== \"undefined\") {\n",
    "#                 var msg = Array.prototype.map.call(arguments, function(x) {\n",
    "#                 return (x && x.nodeName) ? logNode(x) : x;\n",
    "#                 }).join(\" \");\n",
    "#                 dump(\"Reader: (Readability) \" + msg + \"\\n\");\n",
    "#             } else if (typeof console !== \"undefined\") {\n",
    "#                 let args = Array.from(arguments, arg => {\n",
    "#                 if (arg && arg.nodeType == this.ELEMENT_NODE) {\n",
    "#                     return logNode(arg);\n",
    "#                 }\n",
    "#                 return arg;\n",
    "#                 });\n",
    "#                 args.unshift(\"Reader: (Readability)\");\n",
    "#                 console.log.apply(console, args);\n",
    "#             }\n",
    "#             };\n",
    "#         } else {\n",
    "#             this.log = function () {};\n",
    "#         }\n",
    "\n",
    "\n",
    "    def _grabArticle(self, page):\n",
    "        \"\"\"\n",
    "        /***\n",
    "        * grabArticle - Using a variety of metrics (content score, classname, element types), find the content that is\n",
    "        *         most likely to be the stuff a user wants to read. Then return it wrapped up in a div.\n",
    "        *\n",
    "        * @param page a document to run upon. Needs to be a full document, complete with body.\n",
    "        * @return Element\n",
    "        **/\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "    def _postProcessContent(self, articleContent):\n",
    "        \"\"\"\n",
    "        /**\n",
    "        * Run any post-process modifications to article content as necessary.\n",
    "        *\n",
    "        * @param Element\n",
    "        * @return void\n",
    "        **/\n",
    "        \"\"\"\n",
    "        # Readability cannot open relative uris so we convert them to absolute uris.\n",
    "        articleContent = self._fixRelativeUris(articleContent)\n",
    "        articleContent = self._simplifyNestedElements(articleContent)\n",
    "        if not self._keepClasses:\n",
    "            # Remove classes.\n",
    "            articleConent = self._cleanClasses(articleContent)\n",
    "        return articleConent\n",
    "\n",
    "    def _removeNodes(self, nodeList, filterFn = None):\n",
    "        \"\"\"\n",
    "        /**\n",
    "        * Iterates over a NodeList, calls `filterFn` for each node and removes node\n",
    "        * if function returned `true`.\n",
    "        *\n",
    "        * If function is not passed, removes all the nodes in node list.\n",
    "        *\n",
    "        * @param NodeList nodeList The nodes to operate on\n",
    "        * @param Function filterFn the function to use as a filter\n",
    "        * @return void\n",
    "        */\n",
    "        \"\"\"\n",
    "        # Avoid ever operating on live node lists.\n",
    "        # if (self._docJSDOMParser and nodeList[\"_isLiveNodeList\"])\"\"\n",
    "            # raise Exception(\"Do not pass live node lists to _removeNodes\")\n",
    "        \n",
    "        for i in range(len(nodeList) - 1, 0, -1):\n",
    "            node = nodeList[i]\n",
    "            parentNode = node.parent\n",
    "            if parentNode:\n",
    "                if not filterFn or filterFn(node, i, nodeList):\n",
    "                    node.extract()\n",
    "\n",
    "    def _replaceNodeTags(self, nodeList, newTagName):\n",
    "        \"\"\"\n",
    "        /**\n",
    "        * Iterates over a NodeList, and calls _setNodeTag for each node.\n",
    "        *\n",
    "        * @param NodeList nodeList The nodes to operate on\n",
    "        * @param String newTagName the new tag name to use\n",
    "        * @return void\n",
    "        */\n",
    "        \"\"\"\n",
    "        # Avoid ever operating on live node lists.\n",
    "        # if (self._docJSDOMParser and nodeList[\"_isLiveNodeList\"]):\n",
    "            # raise Exception(\"Do not pass live node lists to _replaceNodeTags\")\n",
    "        \n",
    "        for node in nodeList:\n",
    "            self._setNodeTag(node, newTagName) # todo\n",
    "  \n",
    "    def _forEachNode(self, nodeList, fn):\n",
    "        \"\"\"\n",
    "        /**\n",
    "        * Iterate over a NodeList, which doesn't natively fully implement the Array\n",
    "        * interface.\n",
    "        *\n",
    "        * For convenience, the current object context is applied to the provided\n",
    "        * iterate function.\n",
    "        *\n",
    "        * @param  NodeList nodeList The NodeList.\n",
    "        * @param  Function fn       The iterate function.\n",
    "        * @return void\n",
    "        */\n",
    "        \"\"\"\n",
    "        for n in nodeList:\n",
    "            fn(n) # todo - maybe should be n = fn(n)\n",
    "\n",
    "    def _findNode(self, nodeList, fn):\n",
    "        \"\"\"\n",
    "        /**\n",
    "        * Iterate over a NodeList, and return the first node that passes\n",
    "        * the supplied test function\n",
    "        *\n",
    "        * For convenience, the current object context is applied to the provided\n",
    "        * test function.\n",
    "        *\n",
    "        * @param  NodeList nodeList The NodeList.\n",
    "        * @param  Function fn       The test function.\n",
    "        * @return void\n",
    "        */\"\"\"\n",
    "        for n in nodeList:\n",
    "            if fn(n):\n",
    "                return n\n",
    "            \n",
    "    def _someNode(self, nodeList, fn):\n",
    "        \"\"\"\n",
    "        /**\n",
    "        * Iterate over a NodeList, return true if any of the provided iterate\n",
    "        * function calls returns true, false otherwise.\n",
    "        *\n",
    "        * For convenience, the current object context is applied to the\n",
    "        * provided iterate function.\n",
    "        *\n",
    "        * @param  NodeList nodeList The NodeList.\n",
    "        * @param  Function fn       The iterate function.\n",
    "        * @return Boolean\n",
    "        */\n",
    "        \"\"\"\n",
    "        for n in nodeList:\n",
    "            if fn(n):\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "    def _everyNode(self, nodeList, fn):\n",
    "        \"\"\"\n",
    "        /**\n",
    "        * Iterate over a NodeList, return true if all of the provided iterate\n",
    "        * function calls return true, false otherwise.\n",
    "        *\n",
    "        * For convenience, the current object context is applied to the\n",
    "        * provided iterate function.\n",
    "        *\n",
    "        * @param  NodeList nodeList The NodeList.\n",
    "        * @param  Function fn       The iterate function.\n",
    "        * @return Boolean\n",
    "        */\n",
    "        \"\"\"\n",
    "        if [fn(n) for n in nodeList].all():\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def _getAllNodesWithTag(self, node, tagNames):\n",
    "        if node.select:\n",
    "            selector = \",\".join(tagNames)\n",
    "            return node.select(selector)\n",
    "        else:\n",
    "            nodes = []\n",
    "            for tag in tagNames:\n",
    "                collection = node.find_all(tag)\n",
    "                if isinstance(collection, list):\n",
    "                    nodes.extend(collection)\n",
    "                else:\n",
    "                    nodes.append(collection)\n",
    "        return nodes\n",
    "\n",
    "    def _unescapeHtmlEntities(self, str):\n",
    "        \"\"\"\n",
    "        /**\n",
    "        * Converts some of the common HTML entities in string to their corresponding characters.\n",
    "        *\n",
    "        * @param str {string} - a string to unescape.\n",
    "        * @return string without HTML entity.\n",
    "        */\n",
    "        \"\"\"\n",
    "        if not str:\n",
    "            return str\n",
    "\n",
    "        str = unescape(str)\n",
    "        str = re.sub(r\"&(#?[\\w\\d]+);\", lambda match: BeautifulSoup(match.group(0), \"html.parser\").text, str)\n",
    "        return str\n",
    " \n",
    "    def _getArticleMetadata(self, jsonld):\n",
    "        \"\"\"\n",
    "        /**\n",
    "        * Attempts to get excerpt and byline metadata for the article.\n",
    "        *\n",
    "        * @param {Object} jsonld — object containing any metadata that\n",
    "        * could be extracted from JSON-LD object.\n",
    "        *\n",
    "        * @return Object with optional \"excerpt\" and \"byline\" properties\n",
    "        */\n",
    "        \"\"\"\n",
    "        metadata = {}\n",
    "        values = {}\n",
    "        metaElements = self._doc.find_all(\"meta\")\n",
    "\n",
    "        # property is a space-separated list of values\n",
    "        propertyPattern = re.compile(r'\\s*(dc|dcterm|og|twitter)\\s*:\\s*(author|creator|description|title|site_name)\\s*', re.I)\n",
    "        \n",
    "        # name is a single value\n",
    "        namePattern = re.compile(r'^\\s*(?:(dc|dcterm|og|twitter|weibo:(article|webpage))\\s*[\\.:]\\s*)?(author|creator|description|title|site_name)\\s*$', re.I)\n",
    "\n",
    "        # Find description tags\n",
    "        for element in metaElements:\n",
    "            elementName = element.get('name')\n",
    "            elementProperty = element.get('property')\n",
    "            content = element.get('content')\n",
    "            if not content:\n",
    "                continue\n",
    "            matches = None\n",
    "            name = None\n",
    "            if elementProperty:\n",
    "                matches = propertyPattern.match(elementProperty)\n",
    "                if matches:\n",
    "                    # Convert to lowercase, and remove any whitespace\n",
    "                    # so we can match below.\n",
    "                    name = matches.group(0).lower().replace(' ', '')\n",
    "                    # multiple authors\n",
    "                    values[name] = content.strip()\n",
    "            if not matches and elementName and namePattern.match(elementName):\n",
    "                name = elementName\n",
    "                if content:\n",
    "                    # Convert to lowercase, remove any whitespace, and convert dots\n",
    "                    # to colons so we can match below.\n",
    "                    name = name.lower().replace(' ', '').replace('.', ':')\n",
    "                    values[name] = content.strip()\n",
    "\n",
    "        # get title\n",
    "        metadata['title'] = jsonld.get('title') or \\\n",
    "                            values.get('dc:title') or \\\n",
    "                            values.get('dcterm:title') or \\\n",
    "                            values.get('og:title') or \\\n",
    "                            values.get('weibo:article:title') or \\\n",
    "                            values.get('weibo:webpage:title') or \\\n",
    "                            values.get('title') or \\\n",
    "                            values.get('twitter:title')\n",
    "        if not metadata['title']:\n",
    "            metadata['title'] = self._getArticleTitle()\n",
    "\n",
    "        # get author\n",
    "        metadata['byline'] = jsonld.get('byline') or \\\n",
    "                            values.get('dc:creator') or \\\n",
    "                            values.get('dcterm:creator') or \\\n",
    "                            values.get('author')\n",
    "\n",
    "        # get description\n",
    "        metadata['excerpt'] = jsonld.get('excerpt') or \\\n",
    "                            values.get('dc:description') or \\\n",
    "                            values.get('dcterm:description') or \\\n",
    "                            values.get('og:description') or \\\n",
    "                            values.get('weibo:article:description') or \\\n",
    "                            values.get('weibo:webpage:description') or \\\n",
    "                            values.get('description') or \\\n",
    "                            values.get('twitter:description')\n",
    "\n",
    "        # get site name\n",
    "        metadata['siteName'] = jsonld.get('siteName') or \\\n",
    "                                values.get('og:site_name')\n",
    "\n",
    "        # in many sites the meta value is escaped with HTML entities,\n",
    "        # so here we need to unescape it\n",
    "        metadata['title'] = self._unescapeHtmlEntities(metadata['title'])\n",
    "        metadata['byline'] = self._unescapeHtmlEntities(metadata['byline'])\n",
    "        metadata['excerpt'] = self._unescapeHtmlEntities(metadata['excerpt'])\n",
    "        metadata['siteName'] = self._unescapeHtmlEntities(metadata['siteName'])\n",
    "\n",
    "        return metadata\n",
    "        \n",
    "    def _unwrapNoscriptImages(self, doc):\n",
    "        \"\"\"\n",
    "        /**\n",
    "        * Find all <noscript> that are located after <img> nodes, and which contain only one\n",
    "        * <img> element. Replace the first image with the image from inside the <noscript> tag,\n",
    "        * and remove the <noscript> tag. This improves the quality of the images we use on\n",
    "        * some sites (e.g. Medium).\n",
    "        *\n",
    "        * @param Element\n",
    "        **/\n",
    "        \"\"\"\n",
    "        # Find img without source or attributes that might contains image, and remove it.\n",
    "        # This is done to prevent a placeholder img is replaced by img from noscript in next step.\n",
    "        imgs = doc.find_all('img')\n",
    "        for img in imgs:\n",
    "            for attr in img.attrs:\n",
    "                if attr in ['src', 'srcset', 'data-src', 'data-srcset']:\n",
    "                    break\n",
    "                if re.search(r'\\.(jpg|jpeg|png|webp)$', attr):\n",
    "                    break\n",
    "            else:\n",
    "                img.extract()\n",
    "\n",
    "        # Next find noscript and try to extract its image\n",
    "        noscripts = doc.find_all('noscript')\n",
    "        for noscript in noscripts:\n",
    "            # Parse content of noscript and make sure it only contains image\n",
    "            # tmp = BeautifulSoup(noscript.contents[0], 'html.parser')\n",
    "            if not len(noscript.find_all('img')) == 1:\n",
    "                continue\n",
    "\n",
    "            # If noscript has previous sibling and it only contains image,\n",
    "            # replace it with noscript content. However we also keep old\n",
    "            # attributes that might contains image.\n",
    "            prevElement = noscript.previous_sibling\n",
    "            if prevElement and len(prevElement.find_all('img')) == 1:\n",
    "                prevImg = prevElement.find('img')\n",
    "                newImg = noscript.find('img')\n",
    "                for attr, value in prevImg.attrs.items():\n",
    "                    if value == \"\":\n",
    "                        continue\n",
    "                    if attr in ['src', 'srcset'] or re.search(r'\\.(jpg|jpeg|png|webp)$', value):\n",
    "                        if newImg.get(attr) == value:\n",
    "                            continue\n",
    "                        attrName = attr\n",
    "                        if newImg.has_attr(attrName):\n",
    "                            attrName = 'data-old-' + attrName\n",
    "                        newImg[attrName] = value\n",
    "                prevElement.replace_with(noscript.contents[0])\n",
    "\n",
    "    def _removeScripts(doc):\n",
    "        \"\"\"\n",
    "        /**\n",
    "        * Removes script tags from the document.\n",
    "        *\n",
    "        * @param Element\n",
    "        **/\n",
    "        \"\"\"\n",
    "        for script in doc.find_all(['script', 'noscript']):\n",
    "            script.extract()\n",
    "\n",
    "    def _isPhrasingContent(self, node: element.Tag) -> bool:\n",
    "        \"\"\"\n",
    "        /***\n",
    "        * Determine if a node qualifies as phrasing content.\n",
    "        * https://developer.mozilla.org/en-US/docs/Web/Guide/HTML/Content_categories#Phrasing_content\n",
    "        **/\n",
    "        \"\"\"\n",
    "        if node.name is None:  # node is a NavigableString\n",
    "            return True\n",
    "        if node.name == \"br\":\n",
    "            return True\n",
    "        if node.name.upper() in self.PHRASING_ELEMS:\n",
    "            return True\n",
    "        if node.name in [\"a\", \"del\", \"ins\"]:\n",
    "            return all(self._isPhrasingContent(child) for child in node.children)\n",
    "        return False\n",
    "\n",
    "    def _isWhitespace(self, node: element.Tag) -> bool:\n",
    "        if node.name == None:  # node is a NavigableString\n",
    "            return node.strip() == \"\"\n",
    "        if node.name == \"br\":\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _getInnerText(self, e: element.Tag, normalizeSpaces: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        /**\n",
    "        * Get the inner text of a node - cross browser compatibly.\n",
    "        * This also strips out any excess whitespace to be found.\n",
    "        *\n",
    "        * @param Element\n",
    "        * @param Boolean normalizeSpaces (default: true)\n",
    "        * @return string\n",
    "        **/\n",
    "        \"\"\"\n",
    "        textContent = e.get_text().strip()\n",
    "\n",
    "        if normalizeSpaces:\n",
    "            return self.REGEXPS(\"normalize\").sub(\" \", textContent)\n",
    "        return textContent\n",
    "\n",
    "    def _getCharCount(self, e: element.Tag, s: str = \",\") -> int:\n",
    "        \"\"\"\n",
    "        /**\n",
    "        * Get the number of times a string s appears in the node e.\n",
    "        *\n",
    "        * @param Element\n",
    "        * @param string - what to split on. Default is \",\"\n",
    "        * @return number (integer)\n",
    "        **/\n",
    "        \"\"\"\n",
    "        return len(self._getInnerText(e).split(s)) - 1\n",
    "    \n",
    "    def _cleanStyles(self, e: element.Tag) -> None:\n",
    "        if not e or e.name == \"svg\":\n",
    "            return\n",
    "        \n",
    "        # Remove `style` and deprecated presentational attributes\n",
    "        for attr in self.PRESENTATIONAL_ATTRIBUTES:\n",
    "            e.attrs.pop(attr, None)\n",
    "        \n",
    "        if e.name.upper() in self.DEPRECATED_SIZE_ATTRIBUTE_ELEMS:\n",
    "            e.attr.pop(\"width\")\n",
    "            e.attr.pop(\"height\")\n",
    "        \n",
    "        for child in e.children:\n",
    "            if isinstance(child, element.Tag):\n",
    "                self._cleanStyles(child)\n",
    "\n",
    "    def _getLinkDensity(self, element: element.Tag) -> Union[int, float]:\n",
    "        \"\"\"\n",
    "        /**\n",
    "        * Get the density of links as a percentage of the content\n",
    "        * This is the amount of text that is inside a link divided by the total text in the node.\n",
    "        *\n",
    "        * @param Element\n",
    "        * @return number (float)\n",
    "        **/\n",
    "        \"\"\"\n",
    "        textLength = len(self._getInnerText(element))\n",
    "        if textLength == 0:\n",
    "            return 0\n",
    "        \n",
    "        linkLength = 0\n",
    "        for linkNode in element.select(\"a\"):\n",
    "            href = linkNode.get(\"href\")\n",
    "            coefficient = 0.3 if href and self.REGEXPS[\"hashUrl\"].match(href) else 1\n",
    "            linkLength += len(self._getInnerText(linkNode)) * coefficient\n",
    "        \n",
    "        return linkLength / textLength\n",
    "    \n",
    "    def _getClassWeight(self, e: element.Tag) -> int:\n",
    "        \"\"\"\n",
    "        /**\n",
    "        * Get an elements class/id weight. Uses regular expressions to tell if this\n",
    "        * element looks good or bad.\n",
    "        *\n",
    "        * @param Element\n",
    "        * @return number (Integer)\n",
    "        **/\n",
    "        \"\"\"\n",
    "        if not self.flagIsActive(self.FLAG_WEIGHT_CLASSES):\n",
    "            return 0\n",
    "        \n",
    "        weight = 0\n",
    "\n",
    "        # Look for a special classname\n",
    "        class_name = e.get(\"class\")\n",
    "        if class_name and isinstance(class_name, list):\n",
    "            class_name = \" \".join(class_name)\n",
    "            if self.REGEXPS[\"negative\"].search(class_name):\n",
    "                weight -= 25\n",
    "            if self.REGEXPS[\"positive\"].search(class_name):\n",
    "                weight += 25\n",
    "        \n",
    "        # Look for a special ID\n",
    "        element_id = e.get(\"id\")\n",
    "        if element_id:\n",
    "            if self.REGEXPS[\"negative\"].search(element_id):\n",
    "                weight -= 25\n",
    "            if self.REGEXPS[\"positive\"].search(element_id):\n",
    "                weight += 25\n",
    "\n",
    "        return weight\n",
    "\n",
    "    def _getArticleTitle(self):\n",
    "        curTitle = \"\"\n",
    "        origTitle = \"\"\n",
    "\n",
    "        try:\n",
    "            curTitle = origTitle = self._doc.title.string.strip()\n",
    "\n",
    "            # If they had an element with id \"title\" in their HTML\n",
    "            if not isinstance(curTitle, str):\n",
    "                curTitle = origTitle = self._getInnerText(self._doc.find(\"title\"))\n",
    "        except:\n",
    "            # ignore exceptions setting the title.\n",
    "            pass\n",
    "\n",
    "        titleHadHierarchicalSeparators = False\n",
    "\n",
    "        def wordCount(str):\n",
    "            return len(str.split())\n",
    "\n",
    "        # If there's a separator in the title, first remove the final part\n",
    "        if re.search(r' [\\|\\-\\\\\\/>»] ', curTitle):\n",
    "            titleHadHierarchicalSeparators = re.search(r' [\\\\\\/>»] ', curTitle)\n",
    "            curTitle = re.sub(r'(.*)[\\|\\-\\\\\\/>»] .*', r'\\1', origTitle)\n",
    "\n",
    "            # If the resulting title is too short (3 words or fewer), remove\n",
    "            # the first part instead:\n",
    "            if wordCount(curTitle) < 3:\n",
    "                curTitle = re.sub(r'[^\\|\\-\\\\\\/>»]*[\\|\\-\\\\\\/>»](.*)', r'\\1', origTitle)\n",
    "            elif curTitle.find(\": \") != -1:\n",
    "                # Check if we have an heading containing this exact string, so we\n",
    "                # could assume it's the full title.\n",
    "                headings = self._concatNodeLists(\n",
    "                    self._doc.find_all([\"h1\", \"h2\"])\n",
    "                )\n",
    "                trimmedTitle = curTitle.strip()\n",
    "                match = self._someNode(headings, lambda heading: heading.get_text().strip() == trimmedTitle)\n",
    "\n",
    "                # If we don't, let's extract the title out of the original title string.\n",
    "                if not match:\n",
    "                    curTitle = origTitle[origTitle.rfind(\":\") + 1:]\n",
    "\n",
    "                    # If the title is now too short, try the first colon instead:\n",
    "                    if wordCount(curTitle) < 3:\n",
    "                        curTitle = origTitle[origTitle.find(\":\") + 1:]\n",
    "                        # But if we have too many words before the colon there's something weird\n",
    "                        # with the titles and the H tags so let's just use the original title instead\n",
    "                    elif wordCount(origTitle[0:origTitle.find(\":\")]) > 5:\n",
    "                        curTitle = origTitle\n",
    "            elif len(curTitle) > 150 or len(curTitle) < 15:\n",
    "                hOnes = self._doc.find_all(\"h1\")\n",
    "\n",
    "                if len(hOnes) == 1:\n",
    "                    curTitle = self._getInnerText(hOnes[0])\n",
    "\n",
    "            curTitle = re.sub(self.REGEXPS[\"normalize\"], \" \", curTitle.strip())\n",
    "            # If we now have 4 words or fewer as our title, and either no\n",
    "            # 'hierarchical' separators (\\, /, > or ») were found in the original\n",
    "            # title or we decreased the number of words by more than 1 word, use\n",
    "            # the original title.\n",
    "            curTitleWordCount = wordCount(curTitle)\n",
    "            if (curTitleWordCount <= 4 and\n",
    "                (not titleHadHierarchicalSeparators or\n",
    "                curTitleWordCount != wordCount(re.sub(r'[\\|\\-\\\\\\/>»]+', '', origTitle)) - 1)):\n",
    "                curTitle = origTitle\n",
    "            return curTitle\n",
    "\n",
    "    def _prepDocument(self):\n",
    "        \"\"\"\n",
    "        /**\n",
    "        * Prepare the HTML document for readability to scrape it.\n",
    "        * This includes things like stripping javascript, CSS, and handling terrible markup.\n",
    "        *\n",
    "        * @return void\n",
    "        **/\n",
    "        \"\"\"\n",
    "        doc = self._doc\n",
    "\n",
    "        # Remove all style tags in head\n",
    "        self._removeNodes(self._getAllNodesWithTag(doc, [\"style\"]))\n",
    "\n",
    "        if doc.body:\n",
    "            self._replaceBrs(doc.body)\n",
    "\n",
    "        self._replaceNodeTags(self._getAllNodesWithTag(doc, [\"font\"]), \"span\")\n",
    "\n",
    "    def _nextNode(self, node):\n",
    "        \"\"\"\n",
    "        /**\n",
    "        * Finds the next node, starting from the given node, and ignoring\n",
    "        * whitespace in between. If the given node is an element, the same node is\n",
    "        * returned.\n",
    "        */\n",
    "        \"\"\"\n",
    "        next_node = node\n",
    "        while next_node and (next_node.nodeType != self.ELEMENT_NODE) and self.REGEXPS['whitespace'].search(next_node.textContent):\n",
    "            next_node = next_node.nextSibling\n",
    "        return next_node\n",
    "\n",
    "    def _replaceBrs(self, elem):\n",
    "        \"\"\"\n",
    "        /**\n",
    "        * Replaces 2 or more successive <br> elements with a single <p>.\n",
    "        * Whitespace between <br> elements are ignored. For example:\n",
    "        *   <div>foo<br>bar<br> <br><br>abc</div>\n",
    "        * will become:\n",
    "        *   <div>foo<br>bar<p>abc</p></div>\n",
    "        */\n",
    "        \"\"\"\n",
    "        brs = elem.find_all('br')\n",
    "        for br in brs:\n",
    "            next = br.next_sibling\n",
    "\n",
    "            # Whether 2 or more <br> elements have been found and replaced with a\n",
    "            # <p> block.\n",
    "            replaced = False\n",
    "\n",
    "            # If we find a <br> chain, remove the <br>s until we hit another node\n",
    "            # or non-whitespace. This leaves behind the first <br> in the chain\n",
    "            # (which will be replaced with a <p> later).\n",
    "            while next and next.name == 'br':\n",
    "                replaced = True\n",
    "                br_sibling = next.next_sibling\n",
    "                next.extract()\n",
    "                next = br_sibling\n",
    "\n",
    "            # If we removed a <br> chain, replace the remaining <br> with a <p>. Add\n",
    "            # all sibling nodes as children of the <p> until we hit another <br>\n",
    "            # chain.\n",
    "            if replaced:\n",
    "                p = self._doc.new_tag(\"p\")\n",
    "                br.replace_with(p)\n",
    "\n",
    "                next = p.next_sibling\n",
    "                while next:\n",
    "                    # If we've hit another <br><br>, we're done adding children to this <p>.\n",
    "                    if next.name == 'br':\n",
    "                        next_elem = self._nextNode(next.next_sibling)\n",
    "                        if next_elem and next_elem.name == 'br':\n",
    "                            break\n",
    "\n",
    "                    if not self._isPhrasingContent(next):\n",
    "                        break\n",
    "\n",
    "                    # Otherwise, make this node a child of the new <p>.\n",
    "                    sibling = next.next_sibling\n",
    "                    p.append(next)\n",
    "                    next = sibling\n",
    "\n",
    "                while p.last_child and self._is_whitespace(p.last_child):\n",
    "                    p.last_child.extract()\n",
    "\n",
    "                if p.parent.name == \"p\":\n",
    "                    self._setNodeTag(p.parent, \"div\")\n",
    "\n",
    "    def _setNodeTag(self, node, tag):\n",
    "        print(\"_setNodeTag\", node, tag)\n",
    "\n",
    "        replacement = self._doc.new_tag(tag)\n",
    "        while node.contents:\n",
    "            replacement.append(node.contents[0])\n",
    "        node.replace_with(replacement)\n",
    "        \n",
    "        print(\"Will this line error?\")\n",
    "        if node.readability:\n",
    "            replacement.readability = node.readability\n",
    "\n",
    "        for attr, value in node.attrs.items():\n",
    "            try:\n",
    "                replacement[attr] = value\n",
    "            except Exception as ex:\n",
    "                \"\"\"\n",
    "                It's possible for setAttribute() to throw if the attribute name isn't a valid XML Name.\n",
    "                Such attributes can however be parsed from source in HTML docs, see\n",
    "                https://github.com/whatwg/html/issues/4275, so we can hit them here and then throw. We\n",
    "                don't care about such attributes so we ignore them.\n",
    "                \"\"\"\n",
    "                print(\"_setNodetag Error - \", ex)\n",
    "        return replacement\n",
    "    \n",
    "    def parse(self):\n",
    "        # Avoid parsing too large documents, as per configuration option\n",
    "        if self._maxElemsToParse > 0:\n",
    "            numTags = len(self._doc.find_all())\n",
    "            if numTags > self._maxElemsToParse:\n",
    "                raise ValueError(f\"Aborting parsing document; {numTags} elements found\")\n",
    "\n",
    "        # Unwrap image from noscript\n",
    "        self._unwrapNoscriptImages(self._doc)\n",
    "\n",
    "        # Extract JSON-LD metadata before removing scripts\n",
    "        jsonLd = {} # if self._disableJSONLD else self._getJSONLD(self._doc)\n",
    "\n",
    "        # Remove script tags from the document.\n",
    "        # self._removeScripts(this._doc)\n",
    "        for script in self._doc([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "\n",
    "        self._prepDocument()\n",
    "\n",
    "        metadata = self._getArticleMetadata(jsonLd)\n",
    "        self._articleTitle = metadata[\"title\"]\n",
    "        print(self._articleTitle)\n",
    "\n",
    "        articleContent = self._grabArticle()\n",
    "        if not articleContent:\n",
    "            return None\n",
    "\n",
    "        # No fancy logging yet\n",
    "        print(f\"Grabbed: {articleContent.text}\")\n",
    "\n",
    "        self._postProcessContent(articleContent)\n",
    "\n",
    "        # If we haven't found an excerpt in the article's metadata, use the article's\n",
    "        # first paragraph as the excerpt. This is used for displaying a preview of\n",
    "        # the article's content.\n",
    "        if not metadata.excerpt:\n",
    "            paragraphs = articleContent.find_all(\"p\")\n",
    "            if paragraphs:\n",
    "                metadata.excerpt = paragraphs[0].text.strip()\n",
    "\n",
    "        textContent = articleContent.text\n",
    "        return {\n",
    "            \"title\": self._articleTitle,\n",
    "            \"byline\": metadata.byline or self._articleByline,\n",
    "            \"dir\": self._articleDir,\n",
    "            \"lang\": self._articleLang,\n",
    "            \"content\": str(articleContent),\n",
    "            \"textContent\": textContent,\n",
    "            \"length\": len(textContent),\n",
    "            \"excerpt\": metadata.excerpt,\n",
    "            \"siteName\": metadata.siteName or self._articleSiteName,\n",
    "        }\n",
    "        \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://www.builtinla.com/job/engineer/full-stack-software-engineer/132124\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Readability(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_setNodeTag <p><em><span><span><span><span><span><span><span><span><span>*VISA Sponsorship is NOT available for this position*</span></span></span></span></span></span></span></span></span></em><p><span><span>Salary will depend on various factors including applicant's prior relevant job experience, skill set, and geographic location. People Science offers a benefit package for full-time employees which includes: Medical, Dental, Vision, Flexible Spending Account, Life Insurance, DepCare FSA, Flexible Vacation Time Policy, Holidays, and Employee Stock Options. People Science reserves the right to amend, change, alter, and revise pay ranges and benefits offerings at any time. It is at the Company's discretion to determine what pay is provided to a candidate within the range associated with the role.</span></span></p></p> div\n",
      "Will this line error?\n",
      "Full-Stack Software Engineer (Greater LA Area, CA or Remote) - People Science\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_grabArticle() missing 1 required positional argument: 'page'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m reader\u001b[39m.\u001b[39;49mparse()\n",
      "Cell \u001b[0;32mIn[80], line 850\u001b[0m, in \u001b[0;36mReadability.parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_articleTitle \u001b[39m=\u001b[39m metadata[\u001b[39m\"\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    848\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_articleTitle)\n\u001b[0;32m--> 850\u001b[0m articleContent \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_grabArticle()\n\u001b[1;32m    851\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m articleContent:\n\u001b[1;32m    852\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: _grabArticle() missing 1 required positional argument: 'page'"
     ]
    }
   ],
   "source": [
    "reader.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
